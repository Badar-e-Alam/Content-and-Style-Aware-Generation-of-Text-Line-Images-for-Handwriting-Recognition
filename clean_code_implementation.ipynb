{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f4f84b",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cff50875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import torch\n",
    "import numpy as np\n",
    "scale_factor = 2 ** 1\n",
    "IMAGE_HEIGHT = 64\n",
    "IMAGE_WIDTH =  150\n",
    "# device = torch.device(\"cuda\" if torch.cu+2\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "batch_size = 2\n",
    "num_example = 2\n",
    "embedding_size = 64\n",
    "Max_str = 81\n",
    "text_max_len = Max_str + 4\n",
    "vocab = {\n",
    "    \" \",\n",
    "    \"!\",\n",
    "    '\"',\n",
    "    \"#\",\n",
    "    \"&\",\n",
    "    \"'\",\n",
    "    \"(\",\n",
    "    \")\",\n",
    "    \"*\",\n",
    "    \"+\",\n",
    "    \",\",\n",
    "    \"-\",\n",
    "    \".\",\n",
    "    \"/\",\n",
    "    \"0\",\n",
    "    \"1\",\n",
    "    \"2\",\n",
    "    \"3\",\n",
    "    \"4\",\n",
    "    \"5\",\n",
    "    \"6\",\n",
    "    \"7\",\n",
    "    \"8\",\n",
    "    \"9\",\n",
    "    \":\",\n",
    "    \";\",\n",
    "    \"?\",\n",
    "    \"A\",\n",
    "    \"B\",\n",
    "    \"C\",\n",
    "    \"D\",\n",
    "    \"E\",\n",
    "    \"F\",\n",
    "    \"G\",\n",
    "    \"H\",\n",
    "    \"I\",\n",
    "    \"J\",\n",
    "    \"K\",\n",
    "    \"L\",\n",
    "    \"M\",\n",
    "    \"N\",\n",
    "    \"O\",\n",
    "    \"P\",\n",
    "    \"Q\",\n",
    "    \"R\",\n",
    "    \"S\",\n",
    "    \"T\",\n",
    "    \"U\",\n",
    "    \"V\",\n",
    "    \"W\",\n",
    "    \"X\",\n",
    "    \"Y\",\n",
    "    \"Z\",\n",
    "    \"a\",\n",
    "    \"b\",\n",
    "    \"c\",\n",
    "    \"d\",\n",
    "    \"e\",\n",
    "    \"f\",\n",
    "    \"g\",\n",
    "    \"h\",\n",
    "    \"i\",\n",
    "    \"j\",\n",
    "    \"k\",\n",
    "    \"l\",\n",
    "    \"m\",\n",
    "    \"n\",\n",
    "    \"o\",\n",
    "    \"p\",\n",
    "    \"q\",\n",
    "    \"r\",\n",
    "    \"s\",\n",
    "    \"t\",\n",
    "    \"u\",\n",
    "    \"v\",\n",
    "    \"w\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"z\",\n",
    "}\n",
    "\n",
    "cfg = {\n",
    "    \"E\": [\n",
    "        64,\n",
    "        64,\n",
    "        128,\n",
    "        128,\n",
    "        \"M\",\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        256,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        \"M\",\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "        512,\n",
    "    ],\n",
    "}\n",
    "encoder = {data: i for i, data in enumerate(vocab)}\n",
    "decoder = {i: data for i, data in enumerate(vocab)}\n",
    "\"\"\"\n",
    "encoder= {\"A\":0,\"B\":1}\n",
    "decoder={\"0\":A,\"1\":B}\n",
    "\"\"\"\n",
    "tokens = {\"GO_TOKEN\": 0, \"END_TOKEN\": 1, \"PAD_TOKEN\": 2}\n",
    "NUM_WRITERS = 500\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91b1a63",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9129e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_str(data):\n",
    "    # data:str [('hello',\"what\",),(\"on the road\",\"where we are\")]\n",
    "    # data :- lenght of data is dependent on the number_example and the batch_size data[num_examples][batchsize]\n",
    "    data = list(data)\n",
    "    for i in range(len(data)):\n",
    "        # for j in range(len(data[i])):\n",
    "        # data[i] = tuple(s.ljust(text_max_len, \" \") for s in data[i])\n",
    "        if len(data[i]) < text_max_len:\n",
    "            max_str = str()\n",
    "            data[i] += \" \" * (text_max_len - len(data[i]))\n",
    "            # data[i]=max_str\n",
    "        else:\n",
    "            data[i] = data[i]\n",
    "    return tuple(data)\n",
    "\n",
    "\n",
    "def encoding(label, decoder):\n",
    "    # Label[example][batch_size]\n",
    "    words = [\n",
    "        torch.tensor([[decoder[char] for char in word] for word in str1])\n",
    "        for str1 in label\n",
    "    ]\n",
    "    return words  # [examples][batch_size]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e738568d",
   "metadata": {},
   "source": [
    "#### TextEncoder_FC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d926b88",
   "metadata": {},
   "source": [
    "### Visual encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65beb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Visual_encoder(nn.Module):\n",
    "    def __init__(self) :\n",
    "        super(Visual_encoder,self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=100, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.Conv2d(\n",
    "                in_channels=100, out_channels=100, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=100, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        # self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        # self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Shape of the Input in VGG network:-\", x.shape)\n",
    "        x = self.conv1(x.permute(1,0,2,3))\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.upsample1(x)\n",
    "        # x=self.upsample2(x)\n",
    "        # x=self.upsample3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5807fb",
   "metadata": {},
   "source": [
    "### Resnet Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "778496d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class AdaLN(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.rho = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.constant_(self.rho, 0.9)\n",
    "        nn.init.constant_(self.gamma, 1.0)\n",
    "        nn.init.constant_(self.beta, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=[2, 3], keepdim=True)\n",
    "        var = torch.var(x, dim=[2, 3], keepdim=True)\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels,\n",
    "        stride=1,\n",
    "        norm_layer=nn.BatchNorm2d,\n",
    "        activation=F.relu,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            stride=stride,\n",
    "            padding=1,\n",
    "            bias=False,\n",
    "        )\n",
    "        self.bn1 = norm_layer(out_channels)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = norm_layer(out_channels)\n",
    "        self.stride = stride\n",
    "        self.activation = activation\n",
    "        self.adaln = AdaLN(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.adaln(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class DisModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DisModel, self).__init__()\n",
    "        # define the number of layers\n",
    "        self.n_layers = 6\n",
    "        self.final_size = 1024\n",
    "        in_dim = 1\n",
    "        out_dim = 16\n",
    "        self.ff_cc = nn.Conv2d(\n",
    "            in_channels=in_dim,\n",
    "            out_channels=out_dim,\n",
    "            kernel_size=7,\n",
    "            stride=1,\n",
    "            padding=\"same\",\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[\n",
    "                ResidualBlock(\n",
    "                    in_dim,\n",
    "                    out_dim,\n",
    "                )\n",
    "                for _ in range(self.n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.cnn_f = nn.Conv2d(\n",
    "            out_dim, self.final_size, kernel_size=7, stride=1, padding=\"same\"\n",
    "        )\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        ff_cc = self.ff_cc(x)\n",
    "        resnet = self.res_blocks(ff_cc)\n",
    "        output = self.cnn_f(resnet)\n",
    "        return output.squeeze(-1).squeeze(-1)\n",
    "\n",
    "    def calc_dis_fake_loss(self, input_fake):\n",
    "\n",
    "        fake_img = torch.zeros(input_fake.shape[0], self.final_size).to(device)\n",
    "        resp_fake = self.forward(fake_img)\n",
    "        fake_loss = self.bce(resp_fake, fake_img)\n",
    "\n",
    "    def calc_dis_real_loss(self, input_real):\n",
    "        label = torch.ones(input_real.shape[0], self.final_size).to(device=device)\n",
    "        resp_real = self.forward(input_real)\n",
    "        real_loss = self.bce(resp_real, label)\n",
    "        return real_loss\n",
    "\n",
    "    def calc_gen_loss(self, input_fake):\n",
    "        label = torch.ones(input_fake.shape[0], self.final_size).to(device)\n",
    "        resp_fake = self.forward(input_fake)\n",
    "        fake_loss = self.bce(resp_fake, label)\n",
    "        return fake_loss\n",
    "\n",
    "\n",
    "class WriterClaModel(nn.Module):\n",
    "    def __init__(self, num_writers) -> None:\n",
    "        super(WriterClaModel, self).__init__()\n",
    "        self.n_layers = 6\n",
    "        in_dim = 1\n",
    "        out_dim = 16\n",
    "        self.cnn_f = nn.Conv2d(\n",
    "            in_channels=in_dim,\n",
    "            out_channels=out_dim,\n",
    "            kernel_size=7,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            padding_mode=\"reflect\",\n",
    "        )\n",
    "        self.res_blocks = nn.Sequential(\n",
    "             *[\n",
    "                ResidualBlock(\n",
    "                    in_dim,\n",
    "                    out_dim,\n",
    "                )\n",
    "                for _ in range(self.n_layers)\n",
    "            ]\n",
    "        )\n",
    "        self.ff_cc = nn.Conv2d(\n",
    "            in_channels=out_dim,\n",
    "            out_channels=num_writers,\n",
    "            kernel_size=7,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "        )\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        cnn_f = self.cnn_f(x)\n",
    "        resnet = self.res_blocks(cnn_f)\n",
    "        ff_cc = self.ff_cc(resnet)\n",
    "        loss = self.cross_entropy(ff_cc.squeeze(-1).squeeze(-1), y)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class GenModel_FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenModel_FC,self).__init__()\n",
    "        self.enc_image = Visual_encoder().to(device)\n",
    "        self.enc_text = TextEncoder_FC().to(device)\n",
    "        self.dec = Decorder().to(device)\n",
    "        self.linear_mix = nn.Linear(1024, 512)\n",
    "\n",
    "    def decode(self, content, label_text):\n",
    "        # decode content and style codes to an image\n",
    "        self.dec(content, label_text)\n",
    "\n",
    "    # feat_mix: b,1024,8,27\n",
    "    def mix(self, feat_xs, feat_embed):\n",
    "        feat_mix = torch.cat([feat_xs, feat_embed], dim=1)  # b,1024,8,27\n",
    "        f = feat_mix.permute(0, 2, 3, 1)\n",
    "        ff = self.linear_mix(f)  # b,8,27,1024->b,8,27,512\n",
    "        return ff.permute(0, 3, 1, 2)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(\n",
    "        self, class_num, num_res_blocks=4, norm_layer=AdaLN, activation=F.leaky_relu\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.norm_layer = norm_layer\n",
    "        self.activation = activation\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(64)\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[\n",
    "                ResidualBlock(\n",
    "                    64, 64, norm_layer=self.norm_layer, activation=self.activation\n",
    "                )\n",
    "                for _ in range(self.num_res_blocks)\n",
    "            ]\n",
    "        )\n",
    "        self.conv2 = nn.ConvTranspose2d(\n",
    "            64, 32, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False\n",
    "        )\n",
    "        self.bn2 = norm_layer(32)\n",
    "        self.conv3 = nn.ConvTranspose2d(\n",
    "            32, 16, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False\n",
    "        )\n",
    "        self.bn3 = norm_layer(16)\n",
    "        self.conv4 = nn.ConvTranspose2d(\n",
    "            16, 3, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.linear = nn.Linear(3, out_features=class_num)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.linear(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class RecModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RecModel,self).__init__()\n",
    "        self.enc = Encoder()\n",
    "        self.dec = Decorder()\n",
    "\n",
    "    def forward(self, image, text):\n",
    "        visual_out = self.enc(image)\n",
    "        text_visual = self.dec(visual_out, text)\n",
    "        return text_visual\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5c0ec1",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3419071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, infeature, out_feature):\n",
    "        super().__init__()\n",
    "\n",
    "        # Q, K, V weight matrices for each head\n",
    "        self.wq = nn.Linear(infeature, out_feature, bias=False)\n",
    "        self.wk = nn.Linear(infeature, out_feature, bias=False)\n",
    "        self.wv = nn.Linear(infeature, out_feature, bias=False)\n",
    "        self.scale = 1.0 / (infeature ** 0.5)\n",
    "\n",
    "        # Output projection matrix\n",
    "        self.proj = nn.Linear(infeature, out_feature, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_channels* image_height* image_width]\n",
    "        batch, CHW = x.shape\n",
    "        # Reshape input to [batch_size, num_channels*image_height, image_width]\n",
    "        # x = x.reshape(x.size(0), -1, x.size(1))\n",
    "\n",
    "        # Compute Q, K, V matrices for each head\n",
    "        q = self.wq(x)  # q shape: [batch_size, num_channels*image_height, d_model]\n",
    "        k = self.wk(x)  # k shape: [batch_size, num_channels*image_height, d_model]\n",
    "        v = self.wv(x)  # v shape: [batch_size, num_channels*image_height, d_model]\n",
    "        weights = torch.matmul(q, k.transpose(-2, -1))\n",
    "        weights = weights * self.scale\n",
    "        weights = nn.functional.softmax(weights, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(weights, v)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"Multiple heads of the self_attention in parallel\"\n",
    "\n",
    "    def __init__(self, infeature, out_feature, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                Head(infeature=infeature, out_feature=out_feature)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads])\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        print(out.shape)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Cross_attention(nn.Module):\n",
    "    def __init__(self, infeature, out_feature):\n",
    "        super().__init__()\n",
    "\n",
    "        # Q, K, V weight matrices for each head\n",
    "        self.wq = nn.Linear(infeature, out_feature, bias=False)\n",
    "        self.wk = nn.Linear(infeature, out_feature, bias=False)\n",
    "        self.wv = nn.Linear(infeature, out_feature, bias=False)\n",
    "        self.scale = 1.0 / (infeature ** 0.5)\n",
    "\n",
    "        # Output projection matrix\n",
    "        self.proj = nn.Linear(infeature, out_feature, bias=False)\n",
    "\n",
    "    def forward(self, decoder, encoder):\n",
    "        # x shape: [batch_size, num_channels* image_height* image_width]\n",
    "        # batch, CHW = x.shape\n",
    "\n",
    "        # Reshape input to [batch_size, num_channels*image_height, image_width]\n",
    "        # x = x.reshape(x.size(0), -1, x.size(1))\n",
    "\n",
    "        # Compute Q, K, V matrices for each head\n",
    "        q = self.wq(\n",
    "            decoder\n",
    "        )  # q shape: [batch_size, num_channels*image_height, d_model]\n",
    "        k = self.wk(\n",
    "            encoder\n",
    "        )  # k shape: [batch_size, num_channels*image_height, d_model]\n",
    "        v = self.wv(\n",
    "            decoder\n",
    "        )  # v shape: [batch_size, num_channels*image_height, d_model]\n",
    "        weights = torch.matmul(q, k.transpose(-2, -1))\n",
    "        weights = weights * self.scale\n",
    "        weights = nn.functional.softmax(weights, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(weights, v)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MultiHead_CrossAttention(nn.Module):\n",
    "    \"Multiple heads of the self_attention in parallel\"\n",
    "\n",
    "    def __init__(self, infeature, out_feature, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                Cross_attention(infeature=infeature, out_feature=out_feature)\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, encoder, decoder):\n",
    "        out = torch.cat([h.forward(encoder, decoder) for h in self.heads])\n",
    "\n",
    "        out = self.dropout(out)\n",
    "        print(out.shape)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae62042",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6032f955",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Decorder(torch.nn.Module):\n",
    "    def __init__(self, in_feature=32, out_feature=128, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.in_feature = in_feature\n",
    "        self.out_feature = out_feature\n",
    "        self.TextStyle = TextEncoder_FC().to(device)\n",
    "        self.in_feature = embedding_size * IMAGE_HEIGHT * IMAGE_WIDTH\n",
    "        self.linear_upsampling = nn.Linear(\n",
    "            embedding_size * text_max_len, self.in_feature\n",
    "        )\n",
    "        self.linear_downsampling = nn.Linear(\n",
    "            in_features=self.in_feature, out_features=self.out_feature\n",
    "        )\n",
    "        self.block_with_attention = LayerNormLinearDropoutBlock(\n",
    "            in_features=self.in_feature,\n",
    "            out_features=self.out_feature,\n",
    "            num_heads=2,\n",
    "            dropout_prob=0.2,\n",
    "            attention=True,\n",
    "        )\n",
    "        self.block_without_attention = LayerNormLinearDropoutBlock(\n",
    "            in_features=self.in_feature,\n",
    "            out_features=self.out_feature,\n",
    "            num_heads=2,\n",
    "            dropout_prob=0.2,\n",
    "            attention=False,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(self.out_feature)\n",
    "        self.cross_attention = MultiHead_CrossAttention(\n",
    "            infeature=self.out_feature,\n",
    "            out_feature=self.out_feature,\n",
    "            num_heads=2,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.drop = nn.Dropout(self.dropout)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, encoder_out,text_style_content=None):\n",
    "\n",
    "        \n",
    "        char_embedding, global_net = self.TextStyle(text_style_content)\n",
    "        char_upsampling = self.linear_upsampling(char_embedding)\n",
    "        txt_style = global_net + char_upsampling.view(\n",
    "            global_net.size(0),\n",
    "            global_net.size(1),\n",
    "            global_net.size(2),\n",
    "            global_net.size(3),\n",
    "        )\n",
    "        print(f\"{txt_style.shape=}\")\n",
    "        attetion_block, layer_norm = self.block_with_attention(\n",
    "            txt_style.reshape(txt_style.size(0), -1)\n",
    "        )\n",
    "        norm_down_sample = self.linear_downsampling(layer_norm)\n",
    "        norm_down_sample = norm_down_sample.repeat(\n",
    "            attetion_block.size(0) // batch_size, 1\n",
    "        )\n",
    "        attention_norm = attetion_block + norm_down_sample\n",
    "        block_without_attention, _ = self.block_without_attention(attention_norm)\n",
    "        combained_without_attention = block_without_attention + attention_norm\n",
    "        norm = self.norm(combained_without_attention)\n",
    "        cross_attention = self.cross_attention(norm, encoder_out)\n",
    "        drop_out = self.drop(cross_attention)\n",
    "        norm = norm.repeat(drop_out.size(0) // (batch_size + batch_size), 1)\n",
    "        combained_without_attention = drop_out + norm\n",
    "        block_without_attention2, _ = self.block_without_attention(\n",
    "            combained_without_attention\n",
    "        )\n",
    "        final_combained = block_without_attention2 + combained_without_attention\n",
    "\n",
    "        soft_max = self.softmax(final_combained)\n",
    "        return final_combained\n",
    "\n",
    "class LayerNormLinearDropoutBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, in_features, out_features, num_heads, dropout_prob=0.1, attention=False\n",
    "    ):\n",
    "        super(LayerNormLinearDropoutBlock, self).__init__()\n",
    "        self.attention = attention\n",
    "        # Define the layer norm, linear layer, and dropout modules\n",
    "        if self.attention:\n",
    "            self.layer_norm = nn.LayerNorm(in_features)\n",
    "            self.atten = MultiHeadAttention(\n",
    "                in_features, out_features, num_heads, dropout_prob\n",
    "            )\n",
    "        else:\n",
    "            print(\"attention is not applied\")\n",
    "            self.layer_norm = nn.LayerNorm(out_features)\n",
    "            self.linear = nn.Linear(out_features, out_features)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply layer norm to the input tensor\n",
    "        layer_norm = self.layer_norm(x)\n",
    "        if self.attention:\n",
    "            x = self.atten(layer_norm)\n",
    "        else:\n",
    "            # Apply linear transformation to the input tensor\n",
    "\n",
    "            print(\"attention is not applied\")\n",
    "\n",
    "            x = self.linear(layer_norm)\n",
    "\n",
    "        # Apply dropout to the output of the linear layer\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        return x, layer_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8e3d4",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1dff90c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import torch.functional as F\n",
    "\n",
    "# from models.vgg_tro_channel1 import vgg16_bn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        B, C, H, W = (\n",
    "            batch_size,\n",
    "            32,\n",
    "            IMAGE_HEIGHT * scale_factor,\n",
    "            IMAGE_WIDTH * scale_factor,\n",
    "        )\n",
    "        self.num_heads = 4\n",
    "        head_size = 200\n",
    "        print(f\"channel:-{C=},Hight:- {H=} width:- {W=} Batch:- {B=}\")\n",
    "        self.in_feature = 32 * IMAGE_HEIGHT * scale_factor * IMAGE_WIDTH * scale_factor\n",
    "        self.out_feature = 128\n",
    "        self.resnet = Generator_Resnet(class_num=2, num_res_blocks=2).to(device)\n",
    "        self.visual_encoder = Visual_encoder().to(device)  # vgg\n",
    "        self.linear_downsampling = nn.Linear(\n",
    "            in_features=self.in_feature, out_features=self.out_feature\n",
    "        )\n",
    "        self.block_with_attention = LayerNormLinearDropoutBlock(\n",
    "            in_features=self.in_feature,\n",
    "            out_features=self.out_feature,\n",
    "            num_heads=2,\n",
    "            dropout_prob=0.2,\n",
    "            attention=True,\n",
    "        )\n",
    "        self.block_without_attention = LayerNormLinearDropoutBlock(\n",
    "            in_features=self.in_feature,\n",
    "            out_features=self.out_feature,\n",
    "            num_heads=2,\n",
    "            dropout_prob=0.2,\n",
    "            attention=False,\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(self.out_feature)\n",
    "\n",
    "    def forward(self, x):\n",
    "        resent = self.resnet(x.permute(1,0,2,3))  # resent   batch_size,outchannel,Hight , Width\n",
    "\n",
    "        # resent=resent.view(batch_size,-1)\n",
    "        visual_encder = self.visual_encoder(x)  # visual encoder for positionin\n",
    "        # visual_encder=visual_encder.view(batch_size,-1)\n",
    "        print(\n",
    "            f\"Shape of the resent output{resent.shape} and Vgg output shape{visual_encder.shape}\"\n",
    "        )\n",
    "        combained_out = resent + visual_encder  # combained before input\n",
    "        attention_block, norm_layer = self.block_with_attention(\n",
    "            combained_out.view(combained_out.size(0), -1)\n",
    "        )\n",
    "        down_sampled_norm = self.linear_downsampling(norm_layer)\n",
    "        down_sampled_norm = down_sampled_norm.repeat(\n",
    "            attention_block.size(0) // batch_size, 1\n",
    "        )\n",
    "        combained_attention = down_sampled_norm + attention_block\n",
    "        without_attention, _ = self.block_without_attention(combained_attention)\n",
    "        combained_with_attention = combained_attention + without_attention\n",
    "        final_norm = self.norm(combained_with_attention)\n",
    "        print(\"End of encoder\")\n",
    "        return final_norm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "256d60b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "class TextEncoder_FC(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(TextEncoder_FC, self).__init__()\n",
    "        \"\"\"\n",
    "         self.embed = Apply the embedding layer on the text tensor(2,85) -> (batch_size,max_text_len) -> out= (batch_size,max_len,embedding_size)\n",
    "         xx = (batch_size, max_len_embedding_size)\n",
    "         xxx = reshape the embedding output  from (batch_size,max_len_text,embedding_size) -> (batch_size,max_len*embedding_size) \n",
    "         out = Contained the output of the text style_network out_dim -> (batch_size,4096)\n",
    "\n",
    "         xx_new =  apply the Linear layer on the embedding output \n",
    "\n",
    "        \"\"\"\n",
    "        self.embed = nn.Embedding(len(vocab), embedding_size)  # 81,64\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),  # flatten the input tensor to a 1D tensor\n",
    "            nn.Linear(text_max_len * embedding_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(2048, 5440),\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            embedding_size * text_max_len, embedding_size * text_max_len\n",
    "        )  # 64,512\n",
    "        self.linear1 = nn.Linear(embedding_size, embedding_size * text_max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        X: tensor of dim batch_size, max_text_len and embed_dim plz take other things will work accordingly \n",
    "        just take care of it. \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        embedding = self.embed(x.squeeze(-1))  # b,t,embed\n",
    "\n",
    "        batch_size = embedding.shape[0]\n",
    "        xxx = embedding.reshape(batch_size, -1)  # b,t*embed\n",
    "        out = self.fc(xxx)\n",
    "\n",
    "        \"\"\"embed content force\"\"\"\n",
    "        xx_new = self.linear(embedding.view(2, -1)).view(\n",
    "            embedding.size(0), embedding.size(1), embedding.size(2)\n",
    "        )  # b, text_max_len, 512\n",
    "\n",
    "        ts = xx_new.shape[1]  # b,512,8,27\n",
    "        height_reps = IMAGE_HEIGHT  # 8 [-2]\n",
    "        width_reps = max(1, IMAGE_WIDTH // ts)  # [-2] 27\n",
    "        tensor_list = list()\n",
    "        for i in range(ts):\n",
    "            text = [xx_new[:, i : i + 1]]  # b, text_max_len, 512\n",
    "            tmp = torch.cat(text * width_reps, dim=1)\n",
    "            tensor_list.append(tmp)\n",
    "\n",
    "        padding_reps = IMAGE_WIDTH % ts\n",
    "        if padding_reps:\n",
    "            embedded_padding_char = self.embed(torch.full((1, 1), 2, dtype=torch.long,device=device))\n",
    "            # embedded_padding_char = self.linear1(embedded_padding_char)\n",
    "            padding = embedded_padding_char.repeat(batch_size, padding_reps, 1)\n",
    "            tensor_list.append(padding)\n",
    "\n",
    "        res = torch.cat(\n",
    "            tensor_list, dim=1\n",
    "        )  # b, text_max_len * width_reps + padding_reps, 512\n",
    "        res = res.permute(0, 2, 1).unsqueeze(\n",
    "            2\n",
    "        )  # b, 512, 1, text_max_len * width_reps + padding_reps\n",
    "        final_res = torch.cat([res] * height_reps, dim=2)\n",
    "        return out, final_res\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b1536a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size\n",
    "import json\n",
    "from cv2 import imread,resize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3150c1c",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4ab338f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset:\n",
    "    def __init__(\n",
    "        self, base_path=\"Single_Labels\", img_dir=glob.glob(\"Line_data/Images/*/*/*\"),\n",
    "    ):\n",
    "\n",
    "        self.base_path = base_path\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def Load_Image_Label(self, image_path):\n",
    "        # Open the image file\n",
    "        label = tuple()\n",
    "        json_path = os.path.join(\n",
    "            self.base_path, image_path.split(\"\\\\\")[-1][:-4] + \".json\"\n",
    "        )\n",
    "        with open(json_path, \"r\") as json_file:\n",
    "            label = json.load(json_file)\n",
    "        img = imread(image_path, 0)\n",
    "        img = 255 - img\n",
    "        img_height, img_width = img.shape[0], img.shape[1]\n",
    "        n_repeats = int(np.ceil(IMAGE_WIDTH / img_width))\n",
    "        padded_image = np.concatenate([img] * n_repeats, axis=1)\n",
    "        padded_image = padded_image[:IMAGE_HEIGHT, :IMAGE_WIDTH]\n",
    "        resized_img = resize(padded_image, (IMAGE_WIDTH, IMAGE_HEIGHT))\n",
    "        return (resized_img, label)\n",
    "        # plt.imshow(img)\n",
    "        # plt.show()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        Image, Labels = self.Load_Image_Label(self.img_dir[idx])\n",
    "        return torch.tensor(Image, device=device).float(), Labels\n",
    "        # return Image,Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e64e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f9b71a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_loader)=5341     1336=\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "TextDatasetObj = CustomImageDataset()\n",
    "TextDatasetObj = CustomImageDataset()\n",
    "train_ratio = 0.8\n",
    "test_ratio = 1 - train_ratio\n",
    "\n",
    "    # Calculate the sizes of train and test sets based on the split ratios\n",
    "train_size = int(train_ratio * len(TextDatasetObj))\n",
    "test_size = len(TextDatasetObj) - train_size\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "train_set, test_set = random_split(TextDatasetObj, [train_size, test_size])\n",
    "\n",
    "    # Define batch size and number of workers for DataLoader\n",
    "num_workers = 5\n",
    "\n",
    "    # Create DataLoader instances for train and test sets\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, num_workers=num_workers)\n",
    "if len(train_loader) | len(test_loader)==0:\n",
    "    print(\"Data isn't loaded properly\")\n",
    "else:\n",
    "    print(f\"{len(train_loader)=}     {len(test_loader)}=\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9669e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5608b7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "next(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e98afd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_encoder = Visual_encoder().to(device)  # vgg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6ed3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visual_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf05bb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9efa59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2a38a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0cbdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d9c8e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5369a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "448cd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.rand(1,2,10,100)\n",
    "b=torch.rand(2, 64, 10, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0106008d",
   "metadata": {},
   "outputs": [],
   "source": [
    "con=nn.Conv2d(in_channels=64,out_channels=1,kernel_size=1,stride=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f7875e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 10, 100])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=con(b).permute(1,0,2,3)\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4cd9acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat=torch.cat([a,c],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "44a67a89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 10, 100])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple=nn.Conv2d(4,1,1,1)\n",
    "simple(cat).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809922e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "kl_loss = torch.nn.KLDivLoss()\n",
    "\n",
    "# Compute the loss between the input and target tensors\n",
    "loss = kl_loss(F.log_softmax(input_tensor_resized, dim=2), F.softmax(target_tensor_resized, dim=1))\n",
    "\n",
    "print('KLDivLoss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b4dcce3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [85] and output size of (1, 85). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Resize the input and target tensors to a common shape\u001b[39;00m\n\u001b[0;32m     10\u001b[0m new_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m85\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m input_tensor_resized \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_shape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnearest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m target_tensor_resized \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(target_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m), size\u001b[38;5;241m=\u001b[39mnew_shape[\u001b[38;5;241m1\u001b[39m:], mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnearest\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput\u001b[39m\u001b[38;5;124m\"\u001b[39m,input_tensor\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\functional.py:3866\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   3864\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m   3865\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m!=\u001b[39m dim:\n\u001b[1;32m-> 3866\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3867\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput and output must have the same number of spatial dimensions, but got \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3868\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput with spatial dimensions of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and output size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3869\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease provide input tensor in (N, C, d1, d2, ...,dK) format and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3870\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput size in (o1, o2, ...,oK) format.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3871\u001b[0m \n\u001b[0;32m   3872\u001b[0m         )\n\u001b[0;32m   3873\u001b[0m     output_size \u001b[38;5;241m=\u001b[39m size\n\u001b[0;32m   3874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input and output must have the same number of spatial dimensions, but got input with spatial dimensions of [85] and output size of (1, 85). Please provide input tensor in (N, C, d1, d2, ...,dK) format and output size in (o1, o2, ...,oK) format."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define input and target tensors of different sizes\n",
    "input_tensor = torch.randn(2, 1, 85)\n",
    "\n",
    "target_tensor = torch.randn(8, 128)\n",
    "\n",
    "# Resize the input and target tensors to a common shape\n",
    "new_shape = (8, 1, 85)\n",
    "input_tensor_resized = F.interpolate(input_tensor, size=new_shape[1:], mode='nearest')\n",
    "target_tensor_resized = F.interpolate(target_tensor.unsqueeze(0), size=new_shape[1:], mode='nearest').squeeze(0)\n",
    "print(\"Input\",input_tensor.size())\n",
    "print(\"Target\",target_tensor.size)\n",
    "\n",
    "# Instantiate the KLDivLoss object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path=\"Single_Labels\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b36ceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writer_dict(file_data,file):\n",
    "    \n",
    "    \"\"\"\n",
    "        file_data: dict which contains the writer id and line text per image\n",
    "        file: Name of the xml file from which data has been read\n",
    "        function of this function is to take in the dict and write it in the json file with the modified path\n",
    "        (\"a01-000u\\\\a01-000u\"-> a01-000u\\\\a01-000u-00.json\")\n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    for number,dist in enumerate(file_data.items()):\n",
    "        path=os.path.join(base_path,file[17:-4]+\"-\"+f\"{number:02d}\")\n",
    "        with open(path+\".json\",\"w\") as file_json:\n",
    "            json.dump(dist,file_json)\n",
    "def xml_to_text(XML):\n",
    "    \"\"\"\n",
    "        XML: list of the xml files paths \n",
    "        read the  specific text and the writer id and call a writer-dict functions\n",
    "    \"\"\"\n",
    "    for xml_file in tqdm(XML):\n",
    "        tree = ET.parse(source=xml_file, parser=None)\n",
    "        root = tree.getroot()\n",
    "        file_data={}\n",
    "        writer_id=\"\"\n",
    "        for child in root.iter(\"form\"):\n",
    "            data=child.attrib\n",
    "            writer_id=(data[\"writer-id\"])\n",
    "\n",
    "\n",
    "        for child in root.iter(\"line\"):\n",
    "            data = child.attrib\n",
    "            file_data[data[\"text\"]]=writer_id\n",
    "\n",
    "            #file_data[xml_file[0]]=(data[\"text\"],writer_id)\n",
    "        writer_dict(file_data,xml_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296044c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_file=sorted(glob.glob(\"line_data/Labels/*\"))\n",
    "print(\"Total XML Files=\", len(xml_file))\n",
    "len(xml_file)\n",
    "del xml_file[1230] # BAD SAMPLE\n",
    "xml_to_text(xml_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491afaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_Images=glob.glob(\"Line_data/Images/*/*/*\")\n",
    "print(\"Total number of the Images Files=\",len(Images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bf1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "No_xml=(sorted(glob.glob(base_path+\"/*\")))\n",
    "print(f\"Total number of json Files= {len(No_xml)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1c4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.path.join(base_path,Images[0].split(\"\\\\\")[-1][:-4]+\".json\")\n",
    "\"\"\"How to convert the path from .png to .json \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3627392",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac767604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def Load_Image_Label(image_path):\n",
    "    # Open the image file\n",
    "    label=tuple()\n",
    "    json_path=os.path.join(base_path,image_path.split(\"\\\\\")[-1][:-4]+\".json\")\n",
    "    with open(json_path,\"r\") as json_file:\n",
    "        label=json.load(json_file)\n",
    "    img = imread(image_path, 0)\n",
    "    img = 255 - img\n",
    "    img_height, img_width = img.shape[0], img.shape[1]\n",
    "    n_repeats = int(np.ceil(150 / img_width))\n",
    "    padded_image = np.concatenate([img] * n_repeats, axis=1)\n",
    "    padded_image = padded_image[:15, :150]\n",
    "    resized_img = resize(padded_image, (150, 15))\n",
    "    return (resized_img,label)\n",
    "    #plt.imshow(img)\n",
    "    #plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e51b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(Images[92])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d971858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import tqdm\n",
    "from cv2 import imread, resize,imshow,destroyAllWindows,waitKey\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms as transforms\n",
    "\n",
    "import glob,os,sys\n",
    "import json\n",
    "import xml.etree.cElementTree as ET\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from cv2 import imread, resize\n",
    "class CustomImage:\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_path=\"Single_Labels\",\n",
    "        img_dir=glob.glob(\"Line_data/Images/*/*/*\"),\n",
    "        transform=transforms.ToTensor(),\n",
    "    ):\n",
    "\n",
    "        self.base_path=base_path\n",
    "        self.transform = transform\n",
    "        self.img_dir = img_dir\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    def Load_Image_Label(self,image_path):\n",
    "        # Open the image file\n",
    "        label=tuple()\n",
    "        json_path=os.path.join(self.base_path,image_path.split(\"\\\\\")[-1][:-4]+\".json\")\n",
    "        with open(json_path,\"r\") as json_file:\n",
    "            label=json.load(json_file)\n",
    "        img = imread(image_path, 0)\n",
    "        img = 255 - img\n",
    "        img_height, img_width = img.shape[0], img.shape[1]\n",
    "        n_repeats = int(np.ceil(150 / img_width))\n",
    "        padded_image = np.concatenate([img] * n_repeats, axis=1)\n",
    "        padded_image = padded_image[:15, :150]\n",
    "        resized_img = resize(padded_image, (150, 15))\n",
    "        return (resized_img,label)\n",
    "        #plt.imshow(img)\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #import pdb;pdb.set_trace()\n",
    "        Image,Labels=self.Load_Image_Label(self.img_dir[idx])\n",
    "        return torch.tensor(Image, device=\"cpu\").float(), Labels\n",
    "        #return Image,Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded31406",
   "metadata": {},
   "outputs": [],
   "source": [
    " TextDatasetObj = CustomImage()\n",
    "    #no_workers = batch_size // num_example\n",
    "dataset = torch.utils.data.DataLoader(\n",
    "        TextDatasetObj, batch_size=2, shuffle=True, num_workers=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9d7cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = \"Single_Ladhjsdhdsbels\\\\a01-000u-00\"\n",
    "new_path = os.path.join(*path.split(\"\\\\\")[1:])\n",
    "print(new_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a38fb2",
   "metadata": {},
   "source": [
    "# for keys,value in file_data.items():\n",
    "    print({keys:value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f46a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_embd=torch.rand(2,5440\n",
    "                       )\n",
    "from torch import nn\n",
    "linear=nn.Linear(64*85,32)\n",
    "upsample=linear(char_embd)\n",
    "upsample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd193da7",
   "metadata": {},
   "source": [
    "## import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load the XML data from a file\n",
    "tree = ET.parse(xml_file[0])\n",
    "\n",
    "# Get the root element\n",
    "root = tree.getroot()\n",
    "\n",
    "# Find the form element that contains the writer ID\n",
    "form = root.find(\".//form[@type='writer-id']\")\n",
    "print(form)\n",
    "# Get the writer ID attribute value from the form element\n",
    "writer_id = form.attrib['writer-id']\n",
    "\n",
    "# Find the line element with the specified ID\n",
    "line = root.find(\".//line[@id='a01-000u-00']\")\n",
    "\n",
    "# Get the text attribute value from the line element\n",
    "text = line.attrib['text']\n",
    "\n",
    "# Print the results\n",
    "print(\"Writer ID:\", writer_id)\n",
    "print(\"Text:\", text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe97ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(line.get(\"writer-id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a57b952",
   "metadata": {},
   "source": [
    "# Data Structure \n",
    "\n",
    "######  Image: words_data\\\\a01\\\\a01-000u\\\\a01-000u-00-00.png\n",
    "######  Label:labels\\\\a01-000u.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5849a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def XML_TO_JSON():\n",
    "    label_dir = \"labels/\"\n",
    "    label_file = glob.glob(os.path.join(label_dir, \"*.xml\"))\n",
    "    for file in tqdm(label_file):\n",
    "        tree = ET.parse(source=file, parser=None)\n",
    "        root = tree.getroot()\n",
    "        data_dict = dict()\n",
    "        for child in root.iter(\"word\"):\n",
    "            data = child.attrib\n",
    "            data_dict[data[\"id\"]] = data[\"text\"]\n",
    "\n",
    "        with open(os.path.join(\"json/\", file[7:-4] + \".json\"), \"w\") as f:\n",
    "            json.dump(data_dict, f)\n",
    "            f.close()\n",
    "\n",
    "\n",
    "def data_set(path: str, data_name: str):\n",
    "    with open(path, \"rb\") as file:\n",
    "        grades = [x.strip() for x in file.readlines()]\n",
    "    print(\"No. of \" + data_name + \"_Files\", len(grades))\n",
    "    return grades\n",
    "\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13e2b3f",
   "metadata": {},
   "source": [
    "# Path Initilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f524409",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_folder = os.path.join(\"line_data/\")\n",
    "word_folder = glob.glob(base_folder+\"Images/*/*\")\n",
    "json_dict=glob.glob(base_folder+\"Labels/*\")\n",
    "print(\"base data length\",len(word_folder))\n",
    "print(\"json dict length\",(len(json_dict)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68aae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eacc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=list()\n",
    "for i in json_dict:\n",
    "    with open(i) as json_file:\n",
    "        data.append((json.load(json_file)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62f3552",
   "metadata": {},
   "source": [
    "##### creating dictonary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce570073",
   "metadata": {},
   "outputs": [],
   "source": [
    "h=[]\n",
    "import json\n",
    "\n",
    "for lbl in data:\n",
    "    for words in lbl.values():\n",
    "        h.append(words)\n",
    "\n",
    "len(list(set(h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171d263",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_vocab=list(set(h))\n",
    "with open('words_vocab.txt','w') as file:\n",
    "    newline = os.linesep\n",
    "    for i in words_vocab:\n",
    "        file.write(newline+i)\n",
    "\n",
    "lis=list()\n",
    "with open('words_vocab.txt','r') as file:\n",
    "    for line in file:\n",
    "        line = line.rstrip()\n",
    "        lis.append(line)\n",
    "string=[string for string in lis if string!=\"\"]\n",
    "len(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8f046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc455c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1={\"a\":1,\"b\":3,\"c\":5}\n",
    "data2={\"l\":5,\"f\":0,\"h\":10}\n",
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c0cea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab(data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30616df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "vocab={' ',\n",
    " '!',\n",
    " '\"',\n",
    " '#',\n",
    " '&',\n",
    " \"'\",\n",
    " '(',\n",
    " ')',\n",
    " '*',\n",
    " '+',\n",
    " ',',\n",
    " '-',\n",
    " '.',\n",
    " '/',\n",
    " '0',\n",
    " '1',\n",
    " '2',\n",
    " '3',\n",
    " '4',\n",
    " '5',\n",
    " '6',\n",
    " '7',\n",
    " '8',\n",
    " '9',\n",
    " ':',\n",
    " ';',\n",
    " '?',\n",
    " 'A',\n",
    " 'B',\n",
    " 'C',\n",
    " 'D',\n",
    " 'E',\n",
    " 'F',\n",
    " 'G',\n",
    " 'H',\n",
    " 'I',\n",
    " 'J',\n",
    " 'K',\n",
    " 'L',\n",
    " 'M',\n",
    " 'N',\n",
    " 'O',\n",
    " 'P',\n",
    " 'Q',\n",
    " 'R',\n",
    " 'S',\n",
    " 'T',\n",
    " 'U',\n",
    " 'V',\n",
    " 'W',\n",
    " 'X',\n",
    " 'Y',\n",
    " 'Z',\n",
    " 'a',\n",
    " 'b',\n",
    " 'c',\n",
    " 'd',\n",
    " 'e',\n",
    " 'f',\n",
    " 'g',\n",
    " 'h',\n",
    " 'i',\n",
    " 'j',\n",
    " 'k',\n",
    " 'l',\n",
    " 'm',\n",
    " 'n',\n",
    " 'o',\n",
    " 'p',\n",
    " 'q',\n",
    " 'r',\n",
    " 's',\n",
    " 't',\n",
    " 'u',\n",
    " 'v',\n",
    " 'w',\n",
    " 'x',\n",
    " 'y',\n",
    " 'z'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b2435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef27afff",
   "metadata": {},
   "outputs": [],
   "source": [
    "([[set(words) for char in lbl.values() for words in char] for lbl in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004a13b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in json_dict:\n",
    "    data=read_json_files(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53141b3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8ce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image):\n",
    "    #show_image\n",
    "    #img = mpimg.imread(image)\n",
    "    imgplot = plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d46e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "if n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_folder)*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00691059",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data={}\n",
    "count=0\n",
    "max_width=192\n",
    "for image,label in tqdm(zip(word_folder[:10],json_dict[:10])):\n",
    "        folder_data=list()\n",
    "        image_list=glob.glob(os.path.join(image,\"*.png\"))\n",
    "        data=read_json_files(label)\n",
    "        for index,s_image in enumerate(image_list):\n",
    "            tmp_dict=dict()\n",
    "            try:\n",
    "                img= Image.open(s_image)\n",
    "                hight,width=img.size\n",
    "                n_repeats = int(np.ceil(max_width/width))\n",
    "                repeated_image = np.tile(np.array(img), (1, n_repeats, 1))\n",
    "                concatenated_img = np.concatenate(repeated_image,axis=0)\n",
    "                # Convert the numpy array back to an image\n",
    "                #output_img = Image.fromarray(concatenated_img)\n",
    "                #plt.imshow(concatenated_img)\n",
    "                #print(concatenated_img.shape)\n",
    "                #print(f\"{type(img)=}\")\n",
    "                #img=img.resize((h,79))\n",
    "                tmp_dict[\"img\"]=concatenated_img\n",
    "                tmp_dict[\"label\"]=data[index]\n",
    "            except UnidentifiedImageError:\n",
    "                problem_files.append(s_image)\n",
    "            folder_data.append(tmp_dict)\n",
    "            c=str(count)\n",
    "\n",
    "        train_data[c]=folder_data\n",
    "        count+=1\n",
    "           \n",
    "count\n",
    "       \n",
    "\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f8ae00",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[\"0\"][:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5988c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=train_data[\"0\"][:][:1][0][\"img\"]\n",
    "show_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da52629",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data={}\n",
    "count=1231\n",
    "problem_files=[]\n",
    "for image,label in tqdm(zip(word_folder[10:20],json_dict[10:20])):\n",
    "        folder_data=list()\n",
    "        image_list=glob.glob(os.path.join(image,\"*.png\"))\n",
    "        data=read_json_files(label)\n",
    "        for index,s_image in enumerate(image_list):\n",
    "            tmp_dict=dict()\n",
    "            try:\n",
    "                img=Image.open(s_image)\n",
    "                h,w=img.size\n",
    "                img=img.resize((h,79))\n",
    "\n",
    "                #tmp_dict[\"img\"]=img\n",
    "                tmp_dict[\"label\"]=data[index]\n",
    "                img.load()\n",
    "            except UnidentifiedImageError:\n",
    "                problem_files.append(s_image)\n",
    "            folder_data.append(tmp_dict)\n",
    "            c=str(count)\n",
    "        test_data[c]=folder_data\n",
    "        count+=1\n",
    "           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b545cfeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db658014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"No. of samples in train data {len(train_data)} , No. of Samples in test_data {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710e81ea",
   "metadata": {},
   "source": [
    "# dividing dataset into training and test phase\n",
    "#### total dataset 13551  test=2710   train=10841"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1ace00",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set={\"train\":train_data,\"test\":test_data}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0512fd",
   "metadata": {},
   "source": [
    "# Creating pickel file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb70362",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"IMA-32_data_small.pickle\", \"wb\") as file:\n",
    "    pickle.dump(data_set, file, pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fab2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e54ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd hand/files\n",
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fa7c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2849ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset():\n",
    "    def __init__(self, base_path =\"hand/files/IMA-32_data.pickle\" ,  num_examples = 15, target_transform=None):\n",
    "\n",
    "        self.NUM_EXAMPLES = num_examples\n",
    "  \n",
    "        #base_path = DATASET_PATHS\n",
    "        file_to_store = open(base_path, \"rb\")\n",
    "        self.IMG_DATA = pickle.load(file_to_store)['train']\n",
    "        self.IMG_DATA  = dict(list( self.IMG_DATA.items()))#[:NUM_WRITERS])\n",
    "        if 'None' in self.IMG_DATA.keys():\n",
    "            del self.IMG_DATA['None']\n",
    "        self.author_id = list(self.IMG_DATA.keys())\n",
    "\n",
    "        self.transform = get_transform(grayscale=True)\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "        self.collate_fn = TextCollator()\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.author_id)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        \n",
    "\n",
    "        NUM_SAMPLES = self.NUM_EXAMPLES\n",
    "\n",
    "\n",
    "        author_id = self.author_id[index]\n",
    "\n",
    "        self.IMG_DATA_AUTHOR = self.IMG_DATA[author_id]\n",
    "        random_idxs = np.random.choice(len(self.IMG_DATA_AUTHOR), NUM_SAMPLES, replace = True)\n",
    "\n",
    "        rand_id_real = np.random.choice(len(self.IMG_DATA_AUTHOR))\n",
    "        real_img = self.transform(self.IMG_DATA_AUTHOR[rand_id_real]['img'].convert('L'))\n",
    "        real_labels = self.IMG_DATA_AUTHOR[rand_id_real]['label'].encode()\n",
    "\n",
    "\n",
    "        imgs = [np.array(self.IMG_DATA_AUTHOR[idx]['img'].convert('L')) for idx in random_idxs]\n",
    "        labels = [self.IMG_DATA_AUTHOR[idx]['label'].encode() for idx in random_idxs]\n",
    "       \n",
    "        max_width = 192 #[img.shape[1] for img in imgs] \n",
    "        \n",
    "        imgs_pad = []\n",
    "        imgs_wids = []\n",
    "\n",
    "        for img in imgs:\n",
    "\n",
    "            img = 255 - img\n",
    "            img_height, img_width = img.shape[0], img.shape[1]\n",
    "            outImg = np.zeros(( img_height, max_width), dtype='float32')\n",
    "            outImg[:, :img_width] = img[:, :max_width]\n",
    "\n",
    "            img = 255 - outImg\n",
    "\n",
    "            imgs_pad.append(self.transform((Image.fromarray(img))))\n",
    "            imgs_wids.append(img_width)\n",
    "\n",
    "        imgs_pad = torch.cat(imgs_pad, 0)\n",
    "        \n",
    "\n",
    "        item = {'simg': imgs_pad, 'swids':imgs_wids, 'img' : real_img, 'label':real_labels,'img_path':'img_path', 'idx':'indexes', 'wcl':index}\n",
    "\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9bb652",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_store = open(\"IMA-32_data_small.pickle\", \"rb\")\n",
    "IMG_DATA = pickle.load(file_to_store)['train']\n",
    "IMG_DATA  = dict(list( IMG_DATA.items()))#[:NUM_WRITERS])\n",
    "if 'None' in IMG_DATA.keys():\n",
    "    del IMG_DATA['None']\n",
    "author_id = list(IMG_DATA.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe44ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_transform(grayscale=False, convert=True):\n",
    "\n",
    "    transform_list = []\n",
    "    if grayscale:\n",
    "        transform_list.append(transforms.Grayscale(1))\n",
    "\n",
    "    if convert:\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "        if grayscale:\n",
    "            transform_list += [transforms.Normalize((0.5,), (0.5,))]\n",
    "        else:\n",
    "            transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982cfa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_DATA_AUTHOR = IMG_DATA[\"5\"]\n",
    "random_idxs = np.random.choice(len(IMG_DATA_AUTHOR), 5, replace = True)\n",
    "rand_id_real = np.random.choice(len(IMG_DATA_AUTHOR))\n",
    "real_img = (IMG_DATA_AUTHOR[rand_id_real]['image'].convert('L'))\n",
    "real_labels = IMG_DATA_AUTHOR[rand_id_real]['label'].encode()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319264db",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd60433",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6346ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=TextDataset(\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec207c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"words_data/a02/a02-004/a02-004-00-01.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f49e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6bcaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Using cv2.imread() method\n",
    "# Using 0 to read image in grayscale mode\n",
    "img = cv2.imread(img_path, 0)\n",
    "  \n",
    "# Displaying the image\n",
    "print(img)\n",
    "black=1-img/255\n",
    "print((black.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a408f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cv2.imshow('balck', black)\n",
    "cv2.imshow('image', img)\n",
    "key = cv2.waitKey(0)#pauses for 3 seconds before fetching next image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4bc3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b29531",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4f7569",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms((Image.fromarray(img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e8c3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a74af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef36968",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_repeats = int(np.ceil(192/w))\n",
    "n_repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e2be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_image = np.tile(np.array(img), (n_repeats))\n",
    "h,w=repeated_image.shape\n",
    "\n",
    "show_image(repeated_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5871ae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_image = np.concatenate([img] * 10, axis=0)\n",
    "\n",
    "    # Crop the image to the desired width\n",
    "padded_image = padded_image[:, :192]\n",
    "show_image(padded_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=np.arange(10,100).reshape(3,30)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a53ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "[a]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66536c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1=[b'not', b'of', b'the', b'of', b'Mr.', b'ineffectuay', b'and', b'the', b'Shelagh', b'of', b'is', b'terms', b',', b'unit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1c2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab=[]\n",
    "for i in str1:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624d5813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e71f1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=b'unit000000000000000'.decode(\"utf-8\") \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d625c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.tensor(x)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd4dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(50, 50, max_norm=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed76f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max([i for i in str1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8998a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in str1:\n",
    "    print(embedding(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b1d6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "char='*'\n",
    "repeat=0\n",
    "for index,_ in enumerate(str1):\n",
    "    repeat=19-len(str1[index])\n",
    "    str1[index]=(str(str1[index].decode())+\"0\"*repeat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2185ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1=[b'not0000000000000000',\n",
    " b'of00000000000000000',\n",
    " b'the0000000000000000',\n",
    " b'of00000000000000000',\n",
    " b'Mr.0000000000000000',\n",
    " b'ineffectuality00000',\n",
    " b'and0000000000000000',\n",
    " b'the0000000000000000',\n",
    " b'Shelagh000000000000',\n",
    " b'of00000000000000000',\n",
    " b'is00000000000000000',\n",
    " b'terms00000000000000',\n",
    " b',000000000000000000',\n",
    " b'unit000000000000000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401468cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(20-14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d902d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab={'o', 'u', ':', '6', 'g', 'r', '!', 'j', 'c', ',', 'y', 'f', 'b', 'm', '-', '9', '3', 'C', 't', 'v', ')', 'n', '.', 'E', '\"', 'S', \"'\", 'J', 'T', 'w', 'O', 'D', 'P', 'N', 's', 'I', 'h', ';', 'F', 'i', '?', 'k', 'a', 'p', 'W', 'A', 'z', 'M', 'G', 'e', '0', 'd', 'l'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72bab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder={data:i for i,data in enumerate(vocab)}\n",
    "decoder={i:data for i ,data in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d262ff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626eddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e6f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(label,encoder):\n",
    "    lst=[]\n",
    "    lst.append([encoder[char]for lbl in label for char in lbl])\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c9393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder['i']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a25cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding(str1,encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6beb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "\n",
    "class Visual_encoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Visual_encoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=1, out_channels=100, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.Conv2d(\n",
    "                in_channels=100, out_channels=100, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=100, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
    "        # self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        # self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(\"Shape of the Input in VGG network:-\", x.shape)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.upsample1(x)\n",
    "        # x=self.upsample2(x)\n",
    "        # x=self.upsample3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "            \n",
    "class TextEncoder_FC(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(TextEncoder_FC, self).__init__()\n",
    "        \"\"\"\n",
    "         self.embed = Apply the embedding layer on the text tensor(2,85) -> (batch_size,max_text_len) -> out= (batch_size,max_len,embedding_size)\n",
    "         xx = (batch_size, max_len_embedding_size)\n",
    "         xxx = reshape the embedding output  from (batch_size,max_len_text,embedding_size) -> (batch_size,max_len*embedding_size) \n",
    "         out = Contained the output of the text style_network out_dim -> (batch_size,4096)\n",
    "\n",
    "         xx_new =  apply the Linear layer on the embedding output \n",
    "\n",
    "        \"\"\"\n",
    "        embedding_size = 64\n",
    "        Max_str = 81\n",
    "        text_max_len = Max_str + 4#\n",
    "        vocab=81\n",
    "        self.embed = nn.Embedding((vocab), embedding_size)  # 81,64\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Flatten(),  # flatten the input tensor to a 1D tensor\n",
    "            nn.Linear(text_max_len * embedding_size, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.BatchNorm1d(2048),\n",
    "            nn.ReLU(inplace=False),\n",
    "            nn.Linear(2048, 5440),\n",
    "        )\n",
    "        self.linear = nn.Linear(\n",
    "            embedding_size * text_max_len, embedding_size * text_max_len\n",
    "        )  # 64,512\n",
    "        self.linear1 = nn.Linear(embedding_size, embedding_size * text_max_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        X: tensor of dim batch_size, max_text_len and embed_dim plz take other things will work accordingly \n",
    "        just take care of it. \n",
    "        \n",
    "        \"\"\"\n",
    "        xx = self.embed(x.squeeze(-1))  # b,t,embed\n",
    "\n",
    "        batch_size = xx.shape[0]\n",
    "        xxx = xx.reshape(batch_size, -1)  # b,t*embed\n",
    "        out = self.fc(xxx)\n",
    "\n",
    "        \"\"\"embed content force\"\"\"\n",
    "        xx_new = self.linear(xx.view(2, -1)).view(\n",
    "            xx.size(0), xx.size(1), xx.size(2)\n",
    "        )  # b, text_max_len, 512\n",
    "\n",
    "        ts = xx_new.shape[1]  # b,512,8,27\n",
    "        height_reps = IMAGE_HEIGHT  # 8 [-2]\n",
    "        width_reps = max(1, IMAGE_WIDTH // ts)  # [-2] 27\n",
    "        tensor_list = list()\n",
    "        for i in range(ts):\n",
    "            text = [xx_new[:, i : i + 1]]  # b, text_max_len, 512\n",
    "            tmp = torch.cat(text * width_reps, dim=1)\n",
    "            tensor_list.append(tmp)\n",
    "\n",
    "        padding_reps = IMAGE_WIDTH % ts\n",
    "        if padding_reps:\n",
    "            embedded_padding_char = self.embed(torch.full((1, 1), 2, dtype=torch.long,device=device))\n",
    "            # embedded_padding_char = self.linear1(embedded_padding_char)\n",
    "            padding = embedded_padding_char.repeat(batch_size, padding_reps, 1)\n",
    "            tensor_list.append(padding)\n",
    "\n",
    "        res = torch.cat(\n",
    "            tensor_list, dim=1\n",
    "        )  # b, text_max_len * width_reps + padding_reps, 512\n",
    "        res = res.permute(0, 2, 1).unsqueeze(\n",
    "            2\n",
    "        )  # b, 512, 1, text_max_len * width_reps + padding_reps\n",
    "        final_res = torch.cat([res] * height_reps, dim=2)\n",
    "        return out, final_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "856bf233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "a=[1,2,3,4,5]\n",
    "torch.FloatTensor(a)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f900e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenModel_FC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GenModel_FC, self).__init__()\n",
    "        self.enc_image = Visual_encoder().to(device)\n",
    "        self.enc_text = TextEncoder_FC().to(device)\n",
    "        #self.dec = Decoder().to(gpu)\n",
    "        self.linear_mix = nn.Linear(1024, 512)\n",
    "\n",
    "    def decode(self, content, adain_params):\n",
    "        # decode content and style codes to an image\n",
    "        assign_adain_params(adain_params, self.dec)\n",
    "        images = self.dec(content)\n",
    "        return images\n",
    "\n",
    "    # feat_mix: b,1024,8,27\n",
    "    def mix(self, feat_xs, feat_embed):\n",
    "        feat_mix = torch.cat([feat_xs, feat_embed], dim=1) # b,1024,8,27\n",
    "        f = feat_mix.permute(0, 2, 3, 1)\n",
    "        ff = self.linear_mix(f) # b,8,27,1024->b,8,27,512\n",
    "        return ff.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da49ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"line_data/Images/a01/a01-000u/a01-000u-01.png\"\n",
    "img=Image.open(img_path)\n",
    "h,w=img.size\n",
    "show_image(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4628f048",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cv2 import imread, resize, imshow, destroyAllWindows, waitKey\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob, os\n",
    "import json\n",
    "import xml.etree.cElementTree as ET\n",
    "\n",
    "\n",
    "class CustomImageDataset:\n",
    "    def __init__(\n",
    "        self, base_path=\"Single_Labels\", img_dir=glob.glob(\"Line_data/Images/*/*/*\"),\n",
    "    ):\n",
    "\n",
    "        self.base_path = base_path\n",
    "        self.img_dir = img_dir\n",
    "\n",
    "    def Load_Image_Label(self, image_path):\n",
    "        # Open the image file\n",
    "        label = tuple()\n",
    "        json_path = os.path.join(\n",
    "            self.base_path, image_path.split(\"\\\\\")[-1][:-4] + \".json\"\n",
    "        )\n",
    "        with open(json_path, \"r\") as json_file:\n",
    "            label = json.load(json_file)\n",
    "        img = imread(image_path, 0)\n",
    "        img = 255 - img\n",
    "        img_height, img_width = img.shape[0], img.shape[1]\n",
    "        n_repeats = int(np.ceil(15 / img_width))\n",
    "        padded_image = np.concatenate([img] * n_repeats, axis=1)\n",
    "        padded_image = padded_image[:15, :150]\n",
    "        resized_img = resize(padded_image, (15, 150))\n",
    "        return (resized_img, label)\n",
    "        # plt.imshow(img)\n",
    "        # plt.show()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_dir)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # import pdb;pdb.set_trace()\n",
    "        Image, Labels = self.Load_Image_Label(self.img_dir[444])\n",
    "        return torch.tensor(Image, device=device).float(), Labels\n",
    "        # return Image,Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7aad652",
   "metadata": {},
   "outputs": [],
   "source": [
    "TextDatasetObj = CustomImageDataset()\n",
    "dataset = torch.utils.data.DataLoader(\n",
    "        TextDatasetObj, batch_size=10, shuffle=True, num_workers=0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c1d41d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GenModel_FC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen\u001b[38;5;241m=\u001b[39m\u001b[43mGenModel_FC\u001b[49m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'GenModel_FC' is not defined"
     ]
    }
   ],
   "source": [
    "gen=GenModel_FC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e6b7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(1,85).to(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07774a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "<class 'torch.Tensor'>\n",
      "<class 'list'>\n",
      "Mr. Brown, passionate and warm-hearted, led\n",
      "torch.Size([150, 15])\n"
     ]
    }
   ],
   "source": [
    "for img, Label in (dataset):\n",
    "        print()#\n",
    "        print(type(img))#\n",
    "        print((Label))\n",
    "        print((Label[0][0]))\n",
    "        #\n",
    "        print(img[1].shape)\n",
    "        #f_mix = self.gen.mix(f_xs, f_embed)\n",
    "        \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea1b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "85-43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "GenModel_FC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ab2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f9ceae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dic1={'1':23,'3':45}\n",
    "eval(dic1.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625f32dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "s=['1',\"2\",\"3\"]\n",
    "eval(s[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c96512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=4, stride=2, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 1, kernel_size=4, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv2_bn(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ed48d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input=torch.rand((1,8,8))\n",
    "input.unsqueeze(0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c2e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "d=Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c65083",
   "metadata": {},
   "outputs": [],
   "source": [
    "d(input.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf13f51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "52d90d3cc821dd0beedd6e719dbdecc722c226b9d90ed1b663c34e1877f1142e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

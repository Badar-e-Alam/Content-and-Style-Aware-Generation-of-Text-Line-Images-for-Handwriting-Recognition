{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "77dcfcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import torch\n",
    "from lxml import etree\n",
    "from torch import nn\n",
    "import tqdm\n",
    "import numpy as np\n",
    "class CustomImageDataset( ):\n",
    "    def __init__(self, annotations_file=\"E:\\Content and Style Aware Generation\\line_data\\labels\", img_dir=\"E:\\Content and Style Aware Generation\\line_data\\Images\", transform=None, target_transform=None):\n",
    "        self.label_dict={}\n",
    "        self.IMAGE_HEIGHT=10\n",
    "        self.IMAGE_WIDTH=20\n",
    "        self.img_path = img_dir\n",
    "        #self.transform = get_transform(grayscale=True)\n",
    "        self.num_example=10\n",
    "        self.label_path=annotations_file\n",
    "        self.label_dir = os.listdir(self.label_path)\n",
    "        self.img_dir=(glob.glob(self.img_path+\"/*/*/*.png\"))\n",
    "        del(self.label_dir[1230])\n",
    "        for file in tqdm.tqdm(self.label_dir):\n",
    "            tree = ET.parse(source=os.path.join(self.label_path,file), parser=None)\n",
    "            root = tree.getroot()\n",
    "            for child in root.iter(\"line\"):\n",
    "                data = child.attrib\n",
    "                self.label_dict[data[\"id\"]] = data[\"text\"]\n",
    "    def image_lbl(self,index):\n",
    "        img=cv.imread(self.img_dir[index])\n",
    "        img = 255 - img\n",
    "        img_height, img_width = img.shape[0], img.shape[1]\n",
    "        n_repeats = int(np.ceil(self.IMAGE_WIDTH/img_width))\n",
    "        padded_image = np.concatenate([img]*n_repeats, axis=1)\n",
    "        padded_image = padded_image[:self.IMAGE_HEIGHT, :self.IMAGE_WIDTH]\n",
    "        resized_img = cv.resize(padded_image, (self.IMAGE_WIDTH, self.IMAGE_HEIGHT))\n",
    "        label=self.label_dict[os.path.basename(self.img_dir[index])[:-4]]\n",
    "        return resized_img,label\n",
    "        ''' print(padded_image.shape)\n",
    "        plt.imshow(padded_image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print()'''\n",
    "    def __len__(self):\n",
    "        return len(self.label_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        Images=list()\n",
    "        Labels=list()\n",
    "        data_set={}\n",
    "        random_idxs = np.random.choice(len(self.img_dir), 5, replace = True)\n",
    "        \n",
    "        for index in random_idxs:\n",
    "            Img,lbl=self.image_lbl(index)\n",
    "            Images.append(torch.tensor(Img).float().to(device))\n",
    "            #Images.append(torch.nn.functional.pad(tensor, (0, self.IMAGE_WIDTH - tensor.size(2), 0, self.IMAGE_HEIGHT - tensor.size(1))))\n",
    "            Labels.append(lbl)\n",
    "        concate_image=torch.cat(Images,0)   \n",
    "        '''if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)'''\n",
    "        return Images,Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e914e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device=\"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45d68459",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1539/1539 [00:01<00:00, 1226.80it/s]\n"
     ]
    }
   ],
   "source": [
    "data=CustomImageDataset()\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = DataLoader(data, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c04fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fc78bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_encoder(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Visual_encoder, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=3, out_channels=100, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(100),\n",
    "            nn.Conv2d(\n",
    "                in_channels=100, out_channels=100, kernel_size=3, stride=1, padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=100, out_channels=32, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv3 = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv4 = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "        self.conv5 = nn.Conv2d(\n",
    "            in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "     \n",
    "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x=self.upsample1(x)\n",
    "        x=self.upsample2(x)\n",
    "        x=self.upsample3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8518820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AdaLN(nn.Module):\n",
    "    def __init__(self, num_features, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.rho = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.gamma = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.beta = nn.Parameter(torch.Tensor(1, num_features, 1, 1))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.constant_(self.rho, 0.9)\n",
    "        nn.init.constant_(self.gamma, 1.0)\n",
    "        nn.init.constant_(self.beta, 0.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = torch.mean(x, dim=[2, 3], keepdim=True)\n",
    "        var = torch.var(x, dim=[2, 3], keepdim=True)\n",
    "        x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * x + self.beta\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, norm_layer=nn.BatchNorm2d, activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = norm_layer(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = norm_layer(out_channels)\n",
    "        self.stride = stride\n",
    "        self.activation = activation\n",
    "        self.adaln = AdaLN(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.adaln(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "class Generator_Resnet (nn.Module):\n",
    "    def __init__(self,class_num, num_res_blocks=4, norm_layer=AdaLN, activation=F.relu):\n",
    "        super().__init__()\n",
    "        self.num_res_blocks = num_res_blocks\n",
    "        self.norm_layer = norm_layer\n",
    "        self.activation = activation\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=1, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(64)\n",
    "        self.res_blocks = nn.Sequential(*[ResidualBlock(64, 64, norm_layer=self.norm_layer, activation=self.activation) for _ in range(self.num_res_blocks)])\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Second convolutional module\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Third convolutional module\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.upsample4 = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # Fourth convolutional module\n",
    "        self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.upsample5 = nn.Upsample(scale_factor=1, mode='nearest')\n",
    "        \n",
    "        # Final activation layer\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.res_blocks(x)\n",
    "        \n",
    "        x=  self.conv2(x)\n",
    "        x=self.relu2(x)\n",
    "        x=self.upsample2(x)\n",
    "        \n",
    "        x=  self.conv3(x)\n",
    "        x=self.relu3(x)\n",
    "        x=self.upsample3(x)\n",
    "        \n",
    "        x=  self.conv4(x)\n",
    "        x=self.relu4(x)\n",
    "        x=self.upsample4(x)\n",
    "        \n",
    "        x=  self.conv5(x)\n",
    "        x=self.relu5(x)\n",
    "        x=self.upsample5(x)\n",
    "        x = self.tanh(x)\n",
    "        \n",
    "        return x\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69f4ae42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from parameters import *\n",
    "import torch.functional as F\n",
    "from attention import MultiHeadAttention\n",
    "# from models.vgg_tro_channel1 import vgg16_bn\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        B,C,H,W=batch_size,256,IMAGE_HEIGHT*scale_factor,IMAGE_WIDTH*scale_factor\n",
    "        print(f\"channel:-{C=},Hight:- {H=} width:- {W=} Batch:- {B=}\")\n",
    "        self.resnet=Generator_Resnet(class_num=2,num_res_blocks=6)\n",
    "        self.visual_encoder=Visual_encoder()\n",
    "        self.layer_norm=nn.LayerNorm([C,H,W]) #256,\n",
    "        self.attention=MultiHeadAttention(num_heads=8,H=H,W=W,B=B,C=C,scale_factor=scale_factor,head_size=1000,dropout=0.4)\n",
    "        self.dropout=nn.Dropout(p=0.3)\n",
    "        self.linear=nn.Linear(in_features=W*H,out_features=1000,bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        resent=self.resnet(x)   #resent\n",
    "        visual_encder=self.visual_encoder(x)  #visual encoder for positioning\n",
    "        print(f\"Shape of the resent output{resent.shape} and Vgg output shape{visual_encder.shape}\")\n",
    "        combained_out=resent+visual_encder #combained before input\n",
    "        layer_norm=self.layer_norm(combained_out) #layer_norm\n",
    "        attention=self.attention(layer_norm)  #attention layer\n",
    "        dropout=self.dropout(attention)   #dropout layer\n",
    "        norm_dropout=layer_norm+dropout   #combained output of layer_norm and dropout\n",
    "        layer_norm1=self.layer_norm(norm_dropout)  #layer_norm output\n",
    "        linear=self.linear(layer_norm1)     #linear layer\n",
    "        linear_drop=self.dropout(linear)  \n",
    "        norm_dropout1=linear_drop+norm_dropout\n",
    "        final_norm=self.layer_norm(norm_dropout1)\n",
    "\n",
    "        return final_norm\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f87626d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "channel:-C=256,Hight:- H=40 width:- W=80 Batch:- B=2\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3276800000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m network\u001b[38;5;241m=\u001b[39m\u001b[43mEncoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[22], line 17\u001b[0m, in \u001b[0;36mEncoder.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvisual_encoder\u001b[38;5;241m=\u001b[39mVisual_encoder()\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLayerNorm([C,H,W]) \u001b[38;5;66;03m#256,\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m=\u001b[39m\u001b[43mMultiHeadAttention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhead_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(in_features\u001b[38;5;241m=\u001b[39mW\u001b[38;5;241m*\u001b[39mH,out_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mE:\\Content and Style Aware Generation\\data\\attention.py:42\u001b[0m, in \u001b[0;36mMultiHeadAttention.__init__\u001b[1;34m(self, num_heads, H, W, B, C, scale_factor, head_size, dropout)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,num_heads,H,W,B,C,scale_factor,head_size,dropout):\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 42\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList([Head(H\u001b[38;5;241m=\u001b[39mH,W\u001b[38;5;241m=\u001b[39mW,B\u001b[38;5;241m=\u001b[39mB,C\u001b[38;5;241m=\u001b[39mC,Scale_factor\u001b[38;5;241m=\u001b[39mscale_factor,head_size\u001b[38;5;241m=\u001b[39mhead_size,dropout\u001b[38;5;241m=\u001b[39mdropout) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)])\n\u001b[0;32m     43\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(scale_factor\u001b[38;5;241m*\u001b[39mH\u001b[38;5;241m*\u001b[39mW\u001b[38;5;241m*\u001b[39mscale_factor, head_size,)\n\u001b[0;32m     44\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "File \u001b[1;32mE:\\Content and Style Aware Generation\\data\\attention.py:42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,num_heads,H,W,B,C,scale_factor,head_size,dropout):\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m---> 42\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList([\u001b[43mHead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mB\u001b[49m\u001b[43m,\u001b[49m\u001b[43mC\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\u001b[43mScale_factor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale_factor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mhead_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_heads)])\n\u001b[0;32m     43\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mLinear(scale_factor\u001b[38;5;241m*\u001b[39mH\u001b[38;5;241m*\u001b[39mW\u001b[38;5;241m*\u001b[39mscale_factor, head_size,)\n\u001b[0;32m     44\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(dropout)\n",
      "File \u001b[1;32mE:\\Content and Style Aware Generation\\data\\attention.py:16\u001b[0m, in \u001b[0;36mHead.__init__\u001b[1;34m(self, H, W, Scale_factor, head_size, dropout, C, B)\u001b[0m\n\u001b[0;32m     10\u001b[0m final_out\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# print(f\"Height {H=} and Width {W=}\")\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# print(f\"Scale factor values {scale_factor=}\")\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Width,Height=W*scale_factor,H*scale_factor\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# print(f\"{Height=} and {Width=}\")\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mW\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinal_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhead_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(H\u001b[38;5;241m*\u001b[39mW\u001b[38;5;241m*\u001b[39mfinal_out, head_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(H\u001b[38;5;241m*\u001b[39mW\u001b[38;5;241m*\u001b[39mfinal_out, head_size, bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_features \u001b[38;5;241m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features \u001b[38;5;241m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty((out_features, in_features), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(torch\u001b[38;5;241m.\u001b[39mempty(out_features, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 3276800000 bytes."
     ]
    }
   ],
   "source": [
    "network=Encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78d75d81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Image shape torch.Size([2, 3, 20, 10])\n"
     ]
    }
   ],
   "source": [
    "for image,label in dataset:\n",
    "    \n",
    "    #resnet_out=net(image[0].permute(0,3,2,1).to(device))\n",
    "    print(\"Input Image shape\",image[0].permute(0,3,2,1).shape)\n",
    "    ''' print(\"Resnet output shape\",resnet_out.shape)\n",
    "    output=vvg_net(image[0].permute(0,3,2,1).to(device))\n",
    "    print(\"Vvg network outshape\",output.shape)\n",
    "    combained_out=output+resnet_out\n",
    "    print(combained_out.shape)\n",
    "    _,C,W,H=combained_out.shape\n",
    "    print(\"combained shape\",combained_out.shape)\n",
    "    layer=nn.LayerNorm([C,W,H])\n",
    "    out=layer(combained_out)\n",
    "    out.shape'''\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93a51c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_factor=2**3\n",
    "Hight,width=10,20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f7a53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, H,W,Scale_factor,head_size,dropout,C,B):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(Scale_factor*H*W*scale_factor, head_size, bias=False)\n",
    "        self.query = nn.Linear(Scale_factor*H*W*scale_factor, head_size, bias=False)\n",
    "        self.value = nn.Linear(Scale_factor*H*W*scale_factor, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(B,C,C)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,H,W = x.shape \n",
    "        print(\"input shape\",x.shape)\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        print(wei.shape)\n",
    "        print(self.tril.shape)\n",
    "        wei = wei.masked_fill(self.tril[:H, :H] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eee75b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape torch.Size([2, 256, 12800])\n",
      "torch.Size([2, 256, 256])\n",
      "torch.Size([256, 2, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (256) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m attention\u001b[38;5;241m=\u001b[39mHead(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m20\u001b[39m,scale_factor,\u001b[38;5;241m1000\u001b[39m,\u001b[38;5;241m0.20\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m out\u001b[38;5;241m=\u001b[39m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 21\u001b[0m, in \u001b[0;36mHead.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(wei\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtril\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 21\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[43mwei\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmasked_fill\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtril\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mH\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m-inf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m wei \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(wei, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (B, T, T)\u001b[39;00m\n\u001b[0;32m     23\u001b[0m wei \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(wei)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (256) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "attention=Head(10,20,scale_factor,1000,0.20,2,256)\n",
    "out=attention(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e84e667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12800\n"
     ]
    }
   ],
   "source": [
    "sample_tensor=torch.rand(2, 256, 160, 80)\n",
    "sample=sample_tensor.view(2,256,-1)\n",
    "\n",
    "B,C,W,H=sample_tensor.shape\n",
    "l=nn.Linear(W*H,1000,bias=False)\n",
    "\n",
    "print(W*H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ed1456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25e9c6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sample torch.Size([2, 256, 12800])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x12800 and 256x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput sample\u001b[39m\u001b[38;5;124m\"\u001b[39m,sample\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m----> 2\u001b[0m out\u001b[38;5;241m=\u001b[39m\u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      5\u001b[0m drop\u001b[38;5;241m=\u001b[39mnn\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.2\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x12800 and 256x256)"
     ]
    }
   ],
   "source": [
    "print(\"Input sample\",sample.shape)\n",
    "out=l(sample)\n",
    "print(out.shape)\n",
    "\n",
    "drop=nn.Dropout(0.2)\n",
    "out=drop(out)\n",
    "print(out.view(2,256,-1))\n",
    "l=nn.Linear(256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63fd144d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (512x1000 and 256x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (512x1000 and 256x256)"
     ]
    }
   ],
   "source": [
    "l(drop(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24420985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  \"Multiple heads of the self_attention in parallel\"\n",
    "  def __init__(self,num_heads,head_size):\n",
    "    super().__init__()\n",
    "    self.heads=nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "    self.proj=nn.Linear(n_embd,n_embd)\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self,x):\n",
    "    out= torch.cat([h(x) for h in self.heads],dim=-1)\n",
    "    out=self.dropout (self.proj(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd518fa3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3026906",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(out.view(2,-1).shape)\n",
    "print(output.view(2,-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df1ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image[0].permute(0,3,2,1).shape #0,3,2,1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041c236d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=image[0].view(-1,3)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3deee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=out.permute(0,2,3,1).contiguous() #torch.Size([2, 3, 82, 402])\n",
    "pred=result.view(-1,3)\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edf175c",
   "metadata": {},
   "outputs": [],
   "source": [
    " loss = F.cross_entropy(pred,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96147cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input tensors\n",
    "input1 = torch.randn(93670, 3)  # tensor of size [93670, 3]\n",
    "input2 = torch.randn(1508904, 3)  # tensor of size [1508904, 3]\n",
    "print(input1.shape)\n",
    "print(input2.shape)\n",
    "# Define the target labels for each input tensor\n",
    "target1 = torch.randint(0, 3, (93670,))  # tensor of size [93670]\n",
    "print(target1.shape)\n",
    "target2 = torch.randint(0, 3, (1508904//10,))  # tensor of size [1508904//10]\n",
    "print\n",
    "# Repeat target tensor for input tensor 2\n",
    "target2 = target2.repeat(10)\n",
    "\n",
    "# Apply cross-entropy loss to each input tensor\n",
    "loss1 = F.cross_entropy(input1, target1)\n",
    "\n",
    "print(f\"Loss for tensor 1: {loss1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596088c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1, batch_size, self.height//16*512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f042b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.view(-1,out.size(-1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16810789",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.size(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e3a447",
   "metadata": {},
   "outputs": [],
   "source": [
    " loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a2e3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=torch.rand(30030,402)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db785c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# create tensors\n",
    "A = torch.randn(30030, 402)\n",
    "B = torch.randn(3, 2)\n",
    "\n",
    "# transpose B\n",
    "B = torch.transpose(B, 0, 1)\n",
    "\n",
    "# perform matrix multiplication\n",
    "C = torch.mm(A, B)\n",
    "\n",
    "print(C.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2542a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = torch.nn.functional.cross_entropy(t,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291f7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    " loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd4becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_Image_logits(Image,logits):\n",
    "    for img,logit in zip(Image,logits):\n",
    "        plt.title(\"model output\")\n",
    "        plt.imshow(logit.permute(2,1,0).detach().numpy())\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        \n",
    "        plt.title(\"model input\")\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857157f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_Image_logits(image[0],out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b91b94d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# create a tensor of size (2, 32, 10, 20)\n",
    "x = torch.randn(2, 32, 10, 20)\n",
    "\n",
    "# flatten the spatial dimensions\n",
    "x = x.view(2,-1)\n",
    "\n",
    "# create a linear layer\n",
    "linear = nn.Linear(in_features=32*10*20, out_features=100)\n",
    "\n",
    "# apply the linear layer to the flattened tensor\n",
    "output = linear(x)\n",
    "\n",
    "# print the output size\n",
    "print(output.size())  # should be (2, 32, 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "466c46bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6400"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "20*10*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb9a53f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1600])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.rand(2,1600)+torch.rand(2,1600)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8f95e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([16, 32, 100])\n",
      "Normalized shape: torch.Size([16, 32, 100])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import random\n",
    "random.seed(200)\n",
    "# Create a tensor of shape (batch_size, num_features, feature_dim1, feature_dim2)\n",
    "x = torch.randn(16, 32, 10, 10)\n",
    "x=x.view(16,x.size(1),-1)\n",
    "# Apply layer normalization to the last two dimensions\n",
    "layer_norm = nn.LayerNorm([10* 10])\n",
    "x_normalized = layer_norm(x)\n",
    "\n",
    "# Print the shapes of the original and normalized tensors\n",
    "print(\"Original shape:\", x.shape)  # (16, 32, 10, 10)\n",
    "print(\"Normalized shape:\", x_normalized.shape)  # (16, 32, 10, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0f04689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8666, -0.2625,  0.4433, -0.9781, -2.0363,  0.4728, -1.0781, -0.4247,\n",
       "         0.9284, -0.5977])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "34566de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.6208, -0.6617, -1.0732,  0.5020, -0.1845, -0.7720,  0.1638,  0.8357,\n",
       "         1.2288,  1.0207], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_normalized[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "64ac4917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8453, -0.2377,  0.4721, -0.9573, -2.0215,  0.5018, -1.0579, -0.4008,\n",
       "         0.9599, -0.5749], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_normalized[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3f94256a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "        self.query_layer = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.key_layer = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.scale = 1.0 / (hidden_dim ** 0.5)\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Compute query and key vectors\n",
    "        query = self.query_layer(input)\n",
    "        key = self.key_layer(input)\n",
    "\n",
    "        # Compute attention weights using dot product\n",
    "        weights = torch.matmul(query, key.transpose(-2, -1))\n",
    "        weights = weights * self.scale\n",
    "        weights = nn.functional.softmax(weights, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(weights, input)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a95309ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate attention layer with input dimension of 32 and hidden dimension of 64\n",
    "attention_layer = AttentionLayer(32, 64)\n",
    "\n",
    "# Generate a random input tensor of size [batch_size, seq_len, 32]\n",
    "input_tensor = torch.randn(10, 64, 32)\n",
    "\n",
    "# Apply attention layer to input tensor\n",
    "output_tensor = attention_layer(input_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8fa570a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tensor[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "47db7332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1.0 / (100 ** 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4a553821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0.])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros=torch.zeros((10,10))\n",
    "zeros=zeros.view(-1)\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "677abcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones=torch.ones((20,20))\n",
    "ones=ones.view(-1)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7ad0eb26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 100])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(ones.view(4,-1)+zeros).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5cf5706e",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 32, 200]' is invalid for input of size 400",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m200\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Reshape the tensor to size [2, 32, 200]\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Print the new shape of the tensor\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[2, 32, 200]' is invalid for input of size 400"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a tensor of size [2, 200]\n",
    "x = torch.randn(2, 200)\n",
    "\n",
    "# Reshape the tensor to size [2, 32, 200]\n",
    "x = x.view(2, 32, 200)\n",
    "\n",
    "# Print the new shape of the tensor\n",
    "print(x.shape)  # torch.Size([2, 32, 200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f0baa8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 200])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Create a linear layer with the same input and output size\n",
    "linear_layer = nn.Linear(200, 200)\n",
    "activation = nn.Identity()\n",
    "\n",
    "# Create a tensor of size [2, 32, 200]\n",
    "x = torch.randn(2, 32, 200)\n",
    "\n",
    "# Apply the linear layer and activation function to the input tensor\n",
    "x = activation(linear_layer(x))\n",
    "\n",
    "# The output tensor should have the same size as the input tensor\n",
    "print(x.shape)  # torch.Size([2, 32, 200])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bb5be570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32, 400])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.rand(2, 12800).view(2,32,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719c8be2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

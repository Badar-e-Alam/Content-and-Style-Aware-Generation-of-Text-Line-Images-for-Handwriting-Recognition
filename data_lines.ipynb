{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58c7cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2 as cv\n",
    "import glob\n",
    "import torch\n",
    "from lxml import etree\n",
    "import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc635a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_file=\"line_data/labels\",\n",
    "img_dir=\"line_data/images\"\n",
    "img_list=glob.glob(img_dir + \"/*/*/*.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4601259",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a172d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict={}\n",
    "count=0\n",
    "for file in tqdm.tqdm(label_dir):\n",
    "    tree = ET.parse(source=os.path.join(label,file), parser=None)\n",
    "    root = tree.getroot()\n",
    "    count+=1\n",
    "    for child in root.iter(\"line\"):\n",
    "        data = child.attrib\n",
    "        label_dict[data[\"id\"]] = data[\"text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ce25de",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=list()\n",
    "for i in tqdm.tqdm(img_dir):\n",
    "    images.append(cv.imread(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd3a541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012a39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_lbl(index):\n",
    "    img=cv.imread(img_dir[index])\n",
    "    img = 255 - img\n",
    "    img_height, img_width = img.shape[0], img.shape[1]\n",
    "    #rate = float(IMAGE_HEIGHT) / img.shape[0]\n",
    "    #img = cv.resize(img, (int(img.shape[1]*rate)+1, IMAGE_HEIGHT), interpolation=cv.INTER_CUBIC)\n",
    "    ratio = IMAGE_HEIGHT / img_height\n",
    "\n",
    "    # Calculate the new width based on the ratio\n",
    "    n_repeats = int(np.ceil(IMAGE_WIDTH/img_width))\n",
    "    padded_image = np.concatenate([img]*n_repeats, axis=1)    \n",
    "    padded_image = padded_image[:IMAGE_HEIGHT, :IMAGE_WIDTH]\n",
    "    resized_img = cv.resize(padded_image, (IMAGE_WIDTH, desired_height))\n",
    "    '''    print(padded_image.shape)\n",
    "    plt.imshow(padded_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()'''\n",
    "    label=label_dict[os.path.basename(img_dir[index])[:-4]]\n",
    "    return padded_image,label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26c01fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_lbl(16)[1])\n",
    "print(image_lbl(16)[0].shape)\n",
    "plt.imshow(image_lbl(16)[0])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c1a667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_single(file_name):\n",
    "    img = cv.imread(file_name, 0)\n",
    "    if img is None and os.path.exists(file_name):\n",
    "        \n",
    "        # image is present but corrupted\n",
    "        return np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH)), 0\n",
    "\n",
    "    rate = float(IMAGE_HEIGHT) / img.shape[0]\n",
    "    img = cv.resize(img, (int(img.shape[1]*rate)+1, IMAGE_HEIGHT), interpolation=cv.INTER_CUBIC)\n",
    "    img = img/255. # 0-255 -> 0-1\n",
    "\n",
    "    img = 1. - img\n",
    "    img_width = img.shape[-1]\n",
    "\n",
    "    if img_width > IMAGE_WIDTH:\n",
    "        print(\"coming here\")\n",
    "        outImg = img[:, :IMAGE_WIDTH]\n",
    "        img_width = IMAGE_WIDTH\n",
    "    else:\n",
    "        \n",
    "        outImg = np.zeros((IMAGE_HEIGHT, IMAGE_WIDTH), dtype='float32')\n",
    "        outImg[:, :img_width] = img\n",
    "        outImg = outImg.astype('float32')\n",
    "\n",
    "    mean = 0.5\n",
    "    std = 0.5\n",
    "    outImgFinal = (outImg - mean) / std\n",
    "    print(outImgFinal.shape)\n",
    "    return outImgFinal, img_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65be70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img,_=read_image_single(file_name=img_dir[200])\n",
    "print(label_dict[os.path.basename(img_dir[200])[:-4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565bd917",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_idxs = np.random.choice(len(img_dir), 10, replace = True)\n",
    "data=[image_lbl(index) for index in random_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13886322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "\n",
    "def get_transform(grayscale=False, convert=True):\n",
    "\n",
    "    transform_list = []\n",
    "    if grayscale:\n",
    "        transform_list.append(transforms.Grayscale(1))\n",
    "\n",
    "    if convert:\n",
    "        transform_list += [transforms.ToTensor()]\n",
    "        if grayscale:\n",
    "            transform_list += [transforms.Normalize((0.5,), (0.5,))]\n",
    "        else:\n",
    "            transform_list += [transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]\n",
    "\n",
    "    return transforms.Compose(transform_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e2b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the image\n",
    "\n",
    "# Get the original height and width\n",
    "height, width = img.shape[:2]\n",
    "\n",
    "# Define the desired output height\n",
    "desired_height = 324\n",
    "\n",
    "# Calculate the ratio of the new height to the old height\n",
    "ratio = desired_height / height\n",
    "\n",
    "# Calculate the new width based on the ratio\n",
    "desired_width = int(width * ratio)\n",
    "\n",
    "# Resize the image while preserving the aspect ratio\n",
    "resized_img = cv.resize(img, (2270, desired_height))\n",
    "# Save the output image\n",
    "plt.imshow(resized_img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d532d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(json_path)==len(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be76766f",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def imag_label(index):\n",
    "# Reading the data inside the xml\n",
    "# file to a variable under the name\n",
    "# data\n",
    "    IMG_WIDTH=350\n",
    "    IMG_HEIGHT=450\n",
    "    with open(json_path[index], 'r') as f:\n",
    "        data = f.read()\n",
    "    Bs_data = BeautifulSoup(data, \"xml\")\n",
    "    b_unique = Bs_data.find('machine-printed-part')\n",
    "    label=str(b_unique).replace(\"machine-printed-part\",\"\").replace(\"=\",\"\").replace(\"/\",\"\").replace(\"machine-print-line text\",\"\").replace('\\n' , '').replace('<', '').replace('>', '').replace(\"/=\",\"\").replace('\"\"','').replace(\"//\",\"\")\n",
    "    print(label)\n",
    "    print(\"reference image\",image_list[index])\n",
    "    img = cv.imread(image_list[index],0)\n",
    "    print(\"image height and width\",img.shape)\n",
    "    if img is None and os.path.exists(image_list[index]):\n",
    "            # image is present but corrupted\n",
    "            return np.zeros((IMG_HEIGHT, IMG_WIDTH)), 0\n",
    "\n",
    "    rate = float(IMG_HEIGHT) / img.shape[0]\n",
    "    img = cv.resize(img, (int(img.shape[1]*rate)+1, IMG_HEIGHT), interpolation=cv.INTER_CUBIC)\n",
    "    img = img/255. # 0-255 -> 0-1\n",
    "\n",
    "    img = 1. - img\n",
    "    img_width = img.shape[-1]\n",
    "\n",
    "    if img_width > IMG_WIDTH:\n",
    "        outImg = img[:, :IMG_WIDTH]\n",
    "        img_width = IMG_WIDTH\n",
    "    else:\n",
    "        outImg = np.zeros((IMG_HEIGHT, IMG_WIDTH), dtype='float32')\n",
    "        outImg[:, :img_width] = img\n",
    "    outImg = outImg.astype('float32')\n",
    "\n",
    "    mean = 0.5\n",
    "    std = 0.5\n",
    "    outImgFinal = (outImg - mean) / std\n",
    "    plt.imshow(outImgFinal)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de58b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "imag_label(index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5b60db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# create a linear layer with input size 5376 and output size 1024\n",
    "linear = nn.Linear(in_features=5376, out_features=1024)\n",
    "\n",
    "# create a batch of tensors with shape (batch_size, 84, 64)\n",
    "batch_tensor = torch.randn(( 84, 64))\n",
    "\n",
    "# reshape the tensor to (batch_size, input_size)\n",
    "input_tensor = batch_tensor.view(batch_tensor.size(0), -1)\n",
    "\n",
    "# pass the input tensor to the linear layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed427e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch=4\n",
    "n_vocab=85\n",
    "n_embedding=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaea02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = nn.Linear(64, 512)\n",
    "embedding=nn.Embedding(n_vocab,n_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0307bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Height=342\n",
    "Width=2270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5760580",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed=torch.rand(batch,n_vocab,n_embedding)\n",
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad63996",
   "metadata": {},
   "outputs": [],
   "source": [
    "xxx=embed.reshape(batch,-1)\n",
    "xxx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e184c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_new=linear(embed) #4,83,512\n",
    "xx_new.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119e022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=xx_new.shape[1]\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e188731",
   "metadata": {},
   "outputs": [],
   "source": [
    "height_reps=Height\n",
    "width_reps=Width//ts\n",
    "(width_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430ee06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "out=torch.rand(batch,4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82176f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1d8e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list=list()\n",
    "for i in range(ts):\n",
    "    text = [xx_new[:, i:i + 1]] # b, text_max_len, 51281,81,512\n",
    "    tmp = torch.cat(text * int(width_reps), dim=1) #batch,width_reps,512\n",
    "    tensor_list.append(tmp) #ts batch_size,width_reps,512\n",
    "print(len(tensor_list))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b732a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tensor_list)  \n",
    "\"\"\"[-2,-1]\n",
    "                height and width\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c75a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = {'GO_TOKEN': 0, 'END_TOKEN': 1, 'PAD_TOKEN': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6915e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "2270%ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c461ef86",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_reps=2270%ts #10\n",
    "if padding_reps:\n",
    "    print(\"welcome here i dont knwo but welocme\")\n",
    "    char_embeding=embedding(torch.full((1,1),tokens[\"PAD_TOKEN\"],dtype=torch.long)) #(1,1,n_embeding)\n",
    "    char_embeding=linear(char_embeding) #(1,1,512)\n",
    "    padding=char_embeding.repeat(batch,padding_reps,1) ##torch.Size([4, 10, 512])\n",
    "    tensor_list.append(padding)\n",
    "print(len(tensor_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b531bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list[0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cc13c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73a9505",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"batch,text_max,width_reps,+padding,512\n",
    "   bach, ts+1*padding,512\n",
    "\n",
    "\"\"\"\n",
    "res=torch.cat(tensor_list, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560dd37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8788b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = res.permute(0, 2, 1).unsqueeze(2) # b, 512, 1, text_max_len * width_reps + padding_reps\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61cfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res = torch.cat([res] * height_reps, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1650d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3695dfd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "class CustomImageDataset( ):\n",
    "    def __init__(self, annotations_file=\"line_data/labels\", img_dir=\"line_data/images\", transform=None, target_transform=None):\n",
    "        self.label_dict={}\n",
    "        self.IMAGE_HEIGHT=100\n",
    "        self.IMAGE_WIDTH=500\n",
    "        self.img_path = img_dir\n",
    "        #self.transform = get_transform(grayscale=True)\n",
    "        self.num_example=10\n",
    "        self.label_path=annotations_file\n",
    "        self.label_dir = os.listdir(self.label_path)\n",
    "        self.img_dir=(glob.glob(self.img_path+\"/*/*/*.png\"))\n",
    "        del(self.label_dir[1230])\n",
    "        for file in tqdm.tqdm(self.label_dir):\n",
    "            tree = ET.parse(source=os.path.join(self.label_path,file), parser=None)\n",
    "            root = tree.getroot()\n",
    "            for child in root.iter(\"line\"):\n",
    "                data = child.attrib\n",
    "                self.label_dict[data[\"id\"]] = data[\"text\"]\n",
    "    def image_lbl(self,index):\n",
    "        img=cv.imread(self.img_dir[index])\n",
    "        img = 255 - img\n",
    "        img_height, img_width = img.shape[0], img.shape[1]\n",
    "        n_repeats = int(np.ceil(self.IMAGE_WIDTH/img_width))\n",
    "        padded_image = np.concatenate([img]*n_repeats, axis=1)\n",
    "        padded_image = padded_image[:self.IMAGE_HEIGHT, :self.IMAGE_WIDTH]\n",
    "        resized_img = cv.resize(padded_image, (self.IMAGE_WIDTH, self.IMAGE_HEIGHT))\n",
    "        label=self.label_dict[os.path.basename(self.img_dir[index])[:-4]]\n",
    "        return resized_img,label\n",
    "        ''' print(padded_image.shape)\n",
    "        plt.imshow(padded_image)\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print()'''\n",
    "    def __len__(self):\n",
    "        return len(self.label_dict)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        Images=list()\n",
    "        Labels=list()\n",
    "        data_set={}\n",
    "        random_idxs = np.random.choice(len(self.img_dir), 5, replace = True)\n",
    "        \n",
    "        for index in random_idxs:\n",
    "            Img,lbl=self.image_lbl(index)\n",
    "            Images.append(torch.tensor(Img).float())\n",
    "            #Images.append(torch.nn.functional.pad(tensor, (0, self.IMAGE_WIDTH - tensor.size(2), 0, self.IMAGE_HEIGHT - tensor.size(1))))\n",
    "            Labels.append(lbl)\n",
    "        concate_image=torch.cat(Images,0)   \n",
    "        '''if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)'''\n",
    "        return Images,Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d8dc1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mResNET\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# see https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inplanes, planes, stride\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28msuper\u001b[39m(ResNET, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class ResNET(nn.Module):\n",
    "    # see https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html\n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super(ResNET, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7183487",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlocks(nn.Module):\n",
    "    def __init__(self, num_blocks, in_dim,out_dim):\n",
    "        super(ResBlocks, self).__init__()\n",
    "        self.model = []\n",
    "        for i in range(num_blocks):\n",
    "            self.model += [ResNET(in_dim,\n",
    "                                    out_dim,\n",
    "                                    )]\n",
    "        self.model = nn.Sequential(*self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5deb6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13b6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nets=ResBlocks(6,3,512)\n",
    "nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62cc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=CustomImageDataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cce6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "042bdcbb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[43mCustomImageDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n\u001b[0;32m      6\u001b[0m dataset \u001b[38;5;241m=\u001b[39m DataLoader(data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m, in \u001b[0;36mCustomImageDataset.__init__\u001b[1;34m(self, annotations_file, img_dir, transform, target_transform)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_path\u001b[38;5;241m=\u001b[39mannotations_file\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_path)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir\u001b[38;5;241m=\u001b[39m(\u001b[43mglob\u001b[49m\u001b[38;5;241m.\u001b[39mglob(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/*/*/*.png\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_dir[\u001b[38;5;241m1230\u001b[39m])\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_dir):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "data=CustomImageDataset()\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dataset = DataLoader(data, batch_size=5, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cf9293",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe6e6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "1539/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bb63d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "print(len(train_dataset),len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3bd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in train_data_loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a44b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader=DataLoader(train_dataset,batch_size=2,shuffle=True)\n",
    "test_data_loader=DataLoader(test_dataset,batch_size=2,shuffle=True)\n",
    "print(len(train_data_loader),len(test_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(dataset)*0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85666aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for i in tqdm.tqdm(train_dataloader):\n",
    "    count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ea3b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "2, 3, 2270, 342\n",
    "img[i].permute(0,3,2,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7152575b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.rand(2, 342, 2270, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e385b78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in nets.parameters())\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a0eb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in nets.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1d8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=ResNet(in_channels=3, output=512, block_sizes=2)\n",
    "pytorch_total_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "pytorch_total_params\n",
    "nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "30e9e711",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # Q, K, V weight matrices for each head\n",
    "        self.wq = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wk = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.wv = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.scale = 1.0 / (d_model ** 0.5)\n",
    "\n",
    "        # Output projection matrix\n",
    "        self.proj = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, num_channels, image_height, image_width]\n",
    "        batch,channel,high,width=x.shape\n",
    "        # Reshape input to [batch_size, num_channels*image_height, image_width]\n",
    "        x = x.view(x.size(0), -1, x.size(1))\n",
    "        \n",
    "        # Compute Q, K, V matrices for each head\n",
    "        q = self.wq(x)  # q shape: [batch_size, num_channels*image_height, d_model]\n",
    "        k = self.wk(x)  # k shape: [batch_size, num_channels*image_height, d_model]\n",
    "        v = self.wv(x)  # v shape: [batch_size, num_channels*image_height, d_model]\n",
    "        weights = torch.matmul(q, k.transpose(-2, -1))\n",
    "        weights = weights * self.scale\n",
    "        weights = nn.functional.softmax(weights, dim=-1)\n",
    "\n",
    "        # Apply attention weights to values\n",
    "        output = torch.matmul(weights,v )\n",
    "        return output.view(batch,channel,high,width)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "790db186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 200, 32])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(x.size(0), -1, x.size(1)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "682f2e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 32, 20, 10])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Instantiate MultiheadAttention layer with d_model=32 and num_heads=4\n",
    "mha = Head(d_model=32, num_heads=4)\n",
    "\n",
    "# Create input tensor\n",
    "x = torch.randn(2, 32, 20, 10)\n",
    "\n",
    "# Pass input tensor through MultiheadAttention layer\n",
    "out = mha(x)\n",
    "\n",
    "# Print output tensor shape\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c29177e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  \"Multiple heads of the self_attention in parallel\"\n",
    "  def __init__(self,num_heads,dropout):\n",
    "    super().__init__()\n",
    "    self.heads=nn.ModuleList([Head(d_model=32, num_heads=num_heads) for _ in range(num_heads)])\n",
    "    self.dropout=nn.Dropout(dropout)\n",
    "\n",
    "  def forward(self,x):\n",
    "    out= torch.cat([h(x) for h in self.heads])\n",
    "\n",
    "    out=self.dropout (out)\n",
    "    print(out.shape)\n",
    "    return out\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3b951010",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=MultiHeadAttention(4,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8bd3a070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 20, 10])\n"
     ]
    }
   ],
   "source": [
    "out=model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f55b6dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 32, 20, 10])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f60be241",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(24)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = x.repeat(out.size(0)//2, 1, 1, 1)\n",
    "torch.prod(torch.tensor((2,3,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fe6fe60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32, 20, 40])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input tensor\n",
    "input_tensor = torch.randn(8, 32, 20, 40)\n",
    "\n",
    "# Define the linear layer with the same input and output size\n",
    "linear_layer = nn.Linear(40, 40)\n",
    "\n",
    "# Flatten the tensor to make it 2D for the linear layer\n",
    "flatten_tensor = input_tensor.view(-1, 40)\n",
    "\n",
    "# Apply the linear transformation\n",
    "output_tensor = linear_layer(flatten_tensor)\n",
    "\n",
    "# Reshape the output tensor to the original shape\n",
    "output_tensor = output_tensor.view(8, 32, 20, 40)\n",
    "\n",
    "# Print the output tensor shape\n",
    "print(output_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5f51780f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33befcb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
